{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# local code\n",
    "sys.path.insert(1, \"./code/\")\n",
    "from Utils import Utils # student's library\n",
    "from Eval import Eval # student's library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "#train_df.id.nunique()\n",
    "# 19579\n",
    "\n",
    "#train_df.author.unique()\n",
    "# array(['EAP', 'HPL', 'MWS'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 0, 'id': 0, 'text': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Utils.check_for_nulls(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_df = train_df[:20]\n",
    "short_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 36),\n",
       " ('the', 36),\n",
       " ('and', 23),\n",
       " ('i', 19),\n",
       " ('to', 16),\n",
       " ('a', 13),\n",
       " ('his', 12),\n",
       " ('my', 10),\n",
       " ('that', 10),\n",
       " ('in', 10),\n",
       " ('he', 8),\n",
       " ('as', 6),\n",
       " ('with', 5),\n",
       " ('not', 5),\n",
       " ('for', 5),\n",
       " ('this', 4),\n",
       " ('being', 4),\n",
       " ('was', 4),\n",
       " ('all', 4),\n",
       " ('at', 4),\n",
       " ('you', 4),\n",
       " ('could', 4),\n",
       " ('no', 3),\n",
       " ('so', 3),\n",
       " ('from', 3),\n",
       " ('an', 3),\n",
       " ('how', 3),\n",
       " ('we', 3),\n",
       " ('on', 3),\n",
       " ('heart', 3),\n",
       " ('but', 3),\n",
       " ('very', 3),\n",
       " ('had', 3),\n",
       " ('almost', 3),\n",
       " ('her', 3),\n",
       " ('me', 2),\n",
       " ('might', 2),\n",
       " ('its', 2),\n",
       " ('without', 2),\n",
       " ('it', 2),\n",
       " ('never', 2),\n",
       " ('be', 2),\n",
       " ('snuff', 2),\n",
       " ('which,', 2),\n",
       " ('hill,', 2),\n",
       " ('took', 2),\n",
       " ('air', 2),\n",
       " ('looked', 2),\n",
       " ('by', 2),\n",
       " ('even', 2),\n",
       " ('your', 2),\n",
       " ('when', 2),\n",
       " ('paid', 2),\n",
       " ('him', 2),\n",
       " ('felt', 2),\n",
       " ('here', 2),\n",
       " ('say', 2),\n",
       " ('long', 2),\n",
       " ('little', 2),\n",
       " ('eyes', 2),\n",
       " ('nor', 2),\n",
       " ('shall', 2),\n",
       " ('few', 2),\n",
       " ('and,', 2),\n",
       " ('needed', 2),\n",
       " ('native', 2),\n",
       " ('they', 2),\n",
       " ('their', 2),\n",
       " ('well', 2),\n",
       " ('process,', 1),\n",
       " ('however,', 1),\n",
       " ('afforded', 1),\n",
       " ('means', 1),\n",
       " ('ascertaining', 1),\n",
       " ('dimensions', 1),\n",
       " ('dungeon;', 1),\n",
       " ('make', 1),\n",
       " ('circuit,', 1),\n",
       " ('return', 1),\n",
       " ('point', 1),\n",
       " ('whence', 1),\n",
       " ('set', 1),\n",
       " ('out,', 1),\n",
       " ('aware', 1),\n",
       " ('fact;', 1),\n",
       " ('perfectly', 1),\n",
       " ('uniform', 1),\n",
       " ('seemed', 1),\n",
       " ('wall.', 1),\n",
       " ('once', 1),\n",
       " ('occurred', 1),\n",
       " ('fumbling', 1),\n",
       " ('mere', 1),\n",
       " ('mistake.', 1),\n",
       " ('left', 1),\n",
       " ('hand', 1),\n",
       " ('gold', 1),\n",
       " ('box,', 1),\n",
       " ('capered', 1),\n",
       " ('down', 1),\n",
       " ('cutting', 1),\n",
       " ('manner', 1),\n",
       " ('fantastic', 1),\n",
       " ('steps,', 1),\n",
       " ('incessantly', 1),\n",
       " ('greatest', 1),\n",
       " ('possible', 1),\n",
       " ('self', 1),\n",
       " ('satisfaction.', 1),\n",
       " ('lovely', 1),\n",
       " ('is', 1),\n",
       " ('spring', 1),\n",
       " ('windsor', 1),\n",
       " ('terrace', 1),\n",
       " ('sixteen', 1),\n",
       " ('fertile', 1),\n",
       " ('counties', 1),\n",
       " ('spread', 1),\n",
       " ('beneath,', 1),\n",
       " ('speckled', 1),\n",
       " ('happy', 1),\n",
       " ('cottages', 1),\n",
       " ('wealthier', 1),\n",
       " ('towns,', 1),\n",
       " ('former', 1),\n",
       " ('years,', 1),\n",
       " ('cheering', 1),\n",
       " ('fair.', 1),\n",
       " ('finding', 1),\n",
       " ('nothing', 1),\n",
       " ('else,', 1),\n",
       " ('gold,', 1),\n",
       " ('superintendent', 1),\n",
       " ('abandoned', 1),\n",
       " ('attempts;', 1),\n",
       " ('perplexed', 1),\n",
       " ('look', 1),\n",
       " ('occasionally', 1),\n",
       " ('steals', 1),\n",
       " ('over', 1),\n",
       " ('countenance', 1),\n",
       " ('sits', 1),\n",
       " ('thinking', 1),\n",
       " ('desk.', 1),\n",
       " ('youth', 1),\n",
       " ('passed', 1),\n",
       " ('solitude,', 1),\n",
       " ('best', 1),\n",
       " ('years', 1),\n",
       " ('spent', 1),\n",
       " ('under', 1),\n",
       " ('gentle', 1),\n",
       " ('feminine', 1),\n",
       " ('fosterage,', 1),\n",
       " ('has', 1),\n",
       " ('refined', 1),\n",
       " ('groundwork', 1),\n",
       " ('character', 1),\n",
       " ('cannot', 1),\n",
       " ('overcome', 1),\n",
       " ('intense', 1),\n",
       " ('distaste', 1),\n",
       " ('usual', 1),\n",
       " ('brutality', 1),\n",
       " ('exercised', 1),\n",
       " ('board', 1),\n",
       " ('ship:', 1),\n",
       " ('have', 1),\n",
       " ('believed', 1),\n",
       " ('necessary,', 1),\n",
       " ('heard', 1),\n",
       " ('mariner', 1),\n",
       " ('equally', 1),\n",
       " ('noted', 1),\n",
       " ('kindliness', 1),\n",
       " ('respect', 1),\n",
       " ('obedience', 1),\n",
       " ('crew,', 1),\n",
       " ('myself', 1),\n",
       " ('peculiarly', 1),\n",
       " ('fortunate', 1),\n",
       " ('able', 1),\n",
       " ('secure', 1),\n",
       " ('services.', 1),\n",
       " ('astronomer,', 1),\n",
       " ('perhaps,', 1),\n",
       " ('point,', 1),\n",
       " ('refuge', 1),\n",
       " ('suggestion', 1),\n",
       " ('non', 1),\n",
       " ('luminosity;', 1),\n",
       " ('analogy', 1),\n",
       " ('suddenly', 1),\n",
       " ('let', 1),\n",
       " ('fall.', 1),\n",
       " ('surcingle', 1),\n",
       " ('hung', 1),\n",
       " ('ribands', 1),\n",
       " ('body.', 1),\n",
       " ('knew', 1),\n",
       " ('yourself', 1),\n",
       " (\"'stereotomy'\", 1),\n",
       " ('brought', 1),\n",
       " ('think', 1),\n",
       " ('atomies,', 1),\n",
       " ('thus', 1),\n",
       " ('theories', 1),\n",
       " ('epicurus;', 1),\n",
       " ('since,', 1),\n",
       " ('discussed', 1),\n",
       " ('subject', 1),\n",
       " ('ago,', 1),\n",
       " ('mentioned', 1),\n",
       " ('singularly,', 1),\n",
       " ('yet', 1),\n",
       " ('notice,', 1),\n",
       " ('vague', 1),\n",
       " ('guesses', 1),\n",
       " ('noble', 1),\n",
       " ('greek', 1),\n",
       " ('met', 1),\n",
       " ('confirmation', 1),\n",
       " ('late', 1),\n",
       " ('nebular', 1),\n",
       " ('cosmogony,', 1),\n",
       " ('avoid', 1),\n",
       " ('casting', 1),\n",
       " ('upward', 1),\n",
       " ('great', 1),\n",
       " ('nebula', 1),\n",
       " ('orion,', 1),\n",
       " ('certainly', 1),\n",
       " ('expected', 1),\n",
       " ('would', 1),\n",
       " ('do', 1),\n",
       " ('so.', 1),\n",
       " ('confess', 1),\n",
       " ('neither', 1),\n",
       " ('structure', 1),\n",
       " ('languages,', 1),\n",
       " ('code', 1),\n",
       " ('governments,', 1),\n",
       " ('politics', 1),\n",
       " ('various', 1),\n",
       " ('states', 1),\n",
       " ('possessed', 1),\n",
       " ('attractions', 1),\n",
       " ('me.', 1),\n",
       " ('find', 1),\n",
       " ('can', 1),\n",
       " ('feel', 1),\n",
       " ('injuries;', 1),\n",
       " ('learn', 1),\n",
       " ('dread', 1),\n",
       " ('revenge\"', 1),\n",
       " ('days', 1),\n",
       " ('after', 1),\n",
       " ('arrived.', 1),\n",
       " ('barricaded', 1),\n",
       " ('ourselves,', 1),\n",
       " ('present', 1),\n",
       " ('were', 1),\n",
       " ('secure.', 1),\n",
       " ('herbert', 1),\n",
       " ('west', 1),\n",
       " ('fresh', 1),\n",
       " ('bodies', 1),\n",
       " ('because', 1),\n",
       " ('life', 1),\n",
       " ('work', 1),\n",
       " ('reanimation', 1),\n",
       " ('dead.', 1),\n",
       " ('farm', 1),\n",
       " ('like', 1),\n",
       " ('grounds', 1),\n",
       " ('extended', 1),\n",
       " ('back', 1),\n",
       " ('deeply', 1),\n",
       " ('up', 1),\n",
       " ('wheaton', 1),\n",
       " ('street.', 1),\n",
       " ('glance', 1),\n",
       " ('will', 1),\n",
       " ('show', 1),\n",
       " ('fallacy', 1),\n",
       " ('idea.', 1),\n",
       " ('escaped', 1),\n",
       " ('me,', 1),\n",
       " ('must', 1),\n",
       " ('commence', 1),\n",
       " ('destructive', 1),\n",
       " ('endless', 1),\n",
       " ('journey', 1),\n",
       " ('across', 1),\n",
       " ('mountainous', 1),\n",
       " ('ices', 1),\n",
       " ('ocean,', 1),\n",
       " ('amidst', 1),\n",
       " ('cold', 1),\n",
       " ('inhabitants', 1),\n",
       " ('endure', 1),\n",
       " ('which', 1),\n",
       " ('i,', 1),\n",
       " ('genial', 1),\n",
       " ('sunny', 1),\n",
       " ('climate,', 1),\n",
       " ('hope', 1),\n",
       " ('survive.', 1),\n",
       " ('these', 1),\n",
       " ('speeches', 1),\n",
       " ('gave,', 1),\n",
       " ('course,', 1),\n",
       " ('own', 1),\n",
       " ('interpretation;', 1),\n",
       " ('fancying,', 1),\n",
       " ('doubt,', 1),\n",
       " ('events', 1),\n",
       " ('should', 1),\n",
       " ('come', 1),\n",
       " ('into', 1),\n",
       " ('possession', 1),\n",
       " ('vast', 1),\n",
       " ('quantities', 1),\n",
       " ('ready', 1),\n",
       " ('money;', 1),\n",
       " ('provided', 1),\n",
       " ('them', 1),\n",
       " ('owed,', 1),\n",
       " ('trifle', 1),\n",
       " ('more,', 1),\n",
       " ('consideration', 1),\n",
       " ('services,', 1),\n",
       " ('dare', 1),\n",
       " ('cared', 1),\n",
       " ('what', 1),\n",
       " ('became', 1),\n",
       " ('either', 1),\n",
       " ('soul', 1),\n",
       " ('or', 1),\n",
       " ('carcass.', 1),\n",
       " ('sprightliness', 1),\n",
       " ('undue', 1),\n",
       " ('excitement,', 1),\n",
       " ('placid', 1),\n",
       " ('reposed', 1),\n",
       " ('contented', 1),\n",
       " ('love,', 1),\n",
       " ('children,', 1),\n",
       " ('beauty', 1),\n",
       " ('surrounding', 1),\n",
       " ('nature.', 1),\n",
       " ('went', 1),\n",
       " ('far', 1),\n",
       " ('speak', 1),\n",
       " ('slightly', 1),\n",
       " ('hectic', 1),\n",
       " ('cough', 1),\n",
       " ('one', 1),\n",
       " ('time,', 1),\n",
       " ('been', 1),\n",
       " ('troubled', 1),\n",
       " ('chronic', 1),\n",
       " ('rheumatism', 1),\n",
       " ('twinge', 1),\n",
       " ('hereditary', 1),\n",
       " ('gout', 1),\n",
       " ('conclusion,', 1),\n",
       " ('disagreeable', 1),\n",
       " ('inconvenient,', 1),\n",
       " ('hitherto', 1),\n",
       " ('carefully', 1),\n",
       " ('concealed,', 1),\n",
       " ('weakness', 1),\n",
       " ('eyes.', 1),\n",
       " ('facial', 1),\n",
       " ('aspect,', 1),\n",
       " ('too,', 1),\n",
       " ('remarkable', 1),\n",
       " ('maturity;', 1),\n",
       " ('though', 1),\n",
       " ('shared', 1),\n",
       " (\"mother's\", 1),\n",
       " (\"grandfather's\", 1),\n",
       " ('chinlessness,', 1),\n",
       " ('firm', 1),\n",
       " ('precociously', 1),\n",
       " ('shaped', 1),\n",
       " ('nose', 1),\n",
       " ('united', 1),\n",
       " ('expression', 1),\n",
       " ('large,', 1),\n",
       " ('dark,', 1),\n",
       " ('latin', 1),\n",
       " ('give', 1),\n",
       " ('quasi', 1),\n",
       " ('adulthood', 1),\n",
       " ('nigh', 1),\n",
       " ('preternatural', 1),\n",
       " ('intelligence.', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lex = Utils.build_lexicon(short_df.text)   \n",
    "train_lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short_sents = [tokenize(x) for x in list(short_df.text)]\n",
    "# short_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "# http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "\n",
    "# this glove code is wrong type for later work\n",
    "# with open(\"data/glove.42B.300d.txt\", \"rb\") as lines:\n",
    "#     w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "#            for line in lines}\n",
    "\n",
    "\n",
    "# let X be a list of tokenized texts (i.e. list of lists of tokens)\n",
    "# sentences param is token lists\n",
    "\n",
    "# model = gensim.models.Word2Vec(short_sents, size=100)\n",
    "# w2v = dict(zip(model.wv.index2word, model.wv.vectors))\n",
    "# type(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(w2v.keys())[9990:9999]\n",
    "# [b'iconic',\n",
    "#  b'erp',\n",
    "#  b'crest',\n",
    "#  b'radius',\n",
    "#  b'spiral',\n",
    "#  b'nyse',\n",
    "#  b'lotion',\n",
    "#  b'oriental',\n",
    "#  b'admire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_key = list(w2v.keys())[9999]\n",
    "# a_key.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(w2v[str.encode('owl')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "#TaggedDocument does not filter or stem\n",
    "\n",
    "# documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(short_sents)]\n",
    "# model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "\n",
    "# vector = model.infer_vector([\"system\", \"response\"])\n",
    "# vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is it! just use the output of infer_vector in training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15664 17622\n"
     ]
    }
   ],
   "source": [
    "# fast bad data data\n",
    "\n",
    "# regular data\n",
    "#     train: 19580 * .9 rows\n",
    "#     test:  8393 rows\n",
    "#     val:   19580 * .1 rows\n",
    "  \n",
    "VAL_IDX =  math.ceil(len(train_df) * .8)\n",
    "TEST_IDX =  math.ceil(len(train_df) * .9)\n",
    "traindata = train_df[:VAL_IDX]\n",
    "valdata = train_df[VAL_IDX:TEST_IDX]\n",
    "testdata = train_df[TEST_IDX:]\n",
    "\n",
    "print(VAL_IDX, TEST_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_tagged_doc = TaggedDocument(short_sents[0], [0])\n",
    "# example_tagged_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order of ops:\n",
    "# just grammar features\n",
    "# just tfidf vectorizer\n",
    "# both\n",
    "\n",
    "# kmeans\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(traindata.text)\n",
    "Y_train = traindata.author #short_df.author\n",
    "\n",
    "X_test = vectorizer.transform(testdata.text) #short_df.text\n",
    "Y_test = list(testdata.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EAP', 'MWS', 'MWS', ..., 'EAP', 'MWS', 'HPL'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear svc\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lin_clf = LinearSVC()\n",
    "lin_clf.fit(X_train, Y_train) \n",
    "\n",
    "preds = lin_clf.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'list'>\n",
      "Accuracy:  0.8421052631578947\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "\n",
    "accuracy = Eval.get_accuracy(preds, Y_test)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
