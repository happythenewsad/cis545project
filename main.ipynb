{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/peterkong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# local code\n",
    "sys.path.insert(1, \"./code/\")\n",
    "from Utils import Utils # student's library\n",
    "from Eval import Eval # student's library\n",
    "from Extract import Extract # student's library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "#train_df.id.nunique()\n",
    "# 19579\n",
    "\n",
    "#train_df.author.unique()\n",
    "# array(['EAP', 'HPL', 'MWS'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 0, 'id': 0, 'text': 0}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Utils.check_for_nulls(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_df = train_df[:20]\n",
    "short_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data\n",
    "\n",
    "# regular data\n",
    "#     train: 19580 * .9 rows\n",
    "#     test:  8393 rows\n",
    "#     val:   19580 * .1 rows\n",
    "\n",
    "\n",
    "if os.path.isfile('data/traindata.pickle'):\n",
    "    traindata = pd.read_pickle('data/traindata.pickle')\n",
    "    valdata   = pd.read_pickle('data/valdata.pickle')\n",
    "    testdata  = pd.read_pickle('data/testdata.pickle')\n",
    "else: \n",
    "    VAL_IDX  = math.ceil(len(train_df) * .8)\n",
    "    TEST_IDX = math.ceil(len(train_df) * .9)\n",
    "\n",
    "    traindata = train_df[:VAL_IDX]\n",
    "    valdata   = train_df[VAL_IDX:TEST_IDX]\n",
    "    testdata  = train_df[TEST_IDX:]\n",
    "\n",
    "    print(VAL_IDX, TEST_IDX)\n",
    "\n",
    "    traindata.to_pickle('data/traindata.pickle')\n",
    "    valdata.to_pickle('data/valdata.pickle')\n",
    "    testdata.to_pickle('data/testdata.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindata: 15664, valdata: 1958, testdata: 1957\n"
     ]
    }
   ],
   "source": [
    "print(\"traindata: {}, valdata: {}, testdata: {}\".format(len(traindata), len(valdata), len(testdata)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammatical feature engineering \n",
    "# we want to include stopwords here\n",
    "\n",
    "if os.path.isfile('data/train_gram_feats.pickle'):\n",
    "    print(\"reading gram feats from pickle\")\n",
    "    train_gram_feats_df = pd.read_pickle('data/train_gram_feats_df.pickle')\n",
    "    val_gram_feats_df   = pd.read_pickle('data/val_gram_feats_df.pickle')\n",
    "    test_gram_feats_df  = pd.read_pickle('data/test_gram_feats_df.pickle')\n",
    "else:\n",
    "    seq_no = None\n",
    "    train_gram_feats_df = Extract.gram_feats(traindata.text, None, seq_no)\n",
    "\n",
    "    # need to remember so that val/test process\n",
    "    # does not add additional columns\n",
    "    GRAM_FEAT_LIST = list(train_gram_feats_df.columns)\n",
    "\n",
    "    val_gram_feats_df = Extract.gram_feats(valdata.text, GRAM_FEAT_LIST, seq_no)\n",
    "    test_gram_feats_df = Extract.gram_feats(testdata.text, GRAM_FEAT_LIST, seq_no)\n",
    "\n",
    "    # there are 21 columns excluding sequence columns \n",
    "    # 7ary sequence columns can generate up to 2187\n",
    "    \n",
    "    train_gram_feats_df.to_pickle('data/train_gram_feats_df.pickle')\n",
    "    val_gram_feats_df.to_pickle('data/val_gram_feats_df.pickle')\n",
    "    test_gram_feats_df.to_pickle('data/test_gram_feats_df.pickle')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15664, 23)\n",
      "(1958, 23)\n",
      "(1957, 23)\n"
     ]
    }
   ],
   "source": [
    "# removes a singleton feature\n",
    "for df in train_gram_feats_df, val_gram_feats_df, test_gram_feats_df:\n",
    "    if 'SYM_count' in list(df.columns):\n",
    "        df.drop('SYM_count', axis=1, inplace=True)\n",
    "        \n",
    "print(train_gram_feats_df.shape)\n",
    "print(val_gram_feats_df.shape)\n",
    "print(test_gram_feats_df.shape)\n",
    "\n",
    "#set(GRAM_FEAT_LIST) - set(list(val_gram_feats_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15664, 23)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gram_feats_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textual feature engineering\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=STOPWORDS, max_features=100)\n",
    "\n",
    "train_text_feats = vectorizer.fit_transform(traindata.text)\n",
    "Y_train = traindata.author \n",
    "\n",
    "val_text_feats = vectorizer.transform(valdata.text) \n",
    "Y_val = list(valdata.author)\n",
    "\n",
    "test_text_feats = vectorizer.transform(testdata.text) \n",
    "Y_test = list(testdata.author)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15664, 100)\n",
      "(1958, 100)\n",
      "(1957, 100)\n",
      "(15664, 100)\n",
      "(1958, 100)\n",
      "(1957, 100)\n"
     ]
    }
   ],
   "source": [
    "#convert text feats to pandas\n",
    "print(train_text_feats.shape)\n",
    "print(val_text_feats.shape)\n",
    "print(test_text_feats.shape)\n",
    "\n",
    "cols = [\"text_\" + str(x) for x in range(train_text_feats.shape[1])]\n",
    "\n",
    "train_text_feats_df = pd.DataFrame(train_text_feats.todense(), index=None, columns=cols)\n",
    "val_text_feats_df = pd.DataFrame(val_text_feats.todense(), index=None, columns=cols)\n",
    "test_text_feats_df = pd.DataFrame(test_text_feats.todense(), index=None, columns=cols)\n",
    "\n",
    "print(train_text_feats_df.shape)\n",
    "print(val_text_feats_df.shape)\n",
    "print(test_text_feats_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15664, 23)\n",
      "(1958, 23)\n",
      "(1957, 23)\n"
     ]
    }
   ],
   "source": [
    "# concatenating features\n",
    "\n",
    "X_train = train_gram_feats_df.fillna(0)#.join(train_text_feats_df).fillna(0)\n",
    "X_val = val_gram_feats_df.fillna(0)#.join(val_text_feats_df).fillna(0)\n",
    "X_test = test_gram_feats_df.fillna(0)#.join(test_text_feats_df).fillna(0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.sum(axis=0).sort_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity checking\n",
    "assert(list(X_train.columns) == list(X_val.columns))\n",
    "assert(list(X_train.columns) == list(X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "# http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "\n",
    "# this glove code is wrong type for later work\n",
    "# with open(\"data/glove.42B.300d.txt\", \"rb\") as lines:\n",
    "#     w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "#            for line in lines}\n",
    "\n",
    "\n",
    "# let X be a list of tokenized texts (i.e. list of lists of tokens)\n",
    "# sentences param is token lists\n",
    "\n",
    "# model = gensim.models.Word2Vec(short_sents, size=100)\n",
    "# w2v = dict(zip(model.wv.index2word, model.wv.vectors))\n",
    "# type(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(w2v.keys())[9990:9999]\n",
    "# [b'iconic',\n",
    "#  b'erp',\n",
    "#  b'crest',\n",
    "#  b'radius',\n",
    "#  b'spiral',\n",
    "#  b'nyse',\n",
    "#  b'lotion',\n",
    "#  b'oriental',\n",
    "#  b'admire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_key = list(w2v.keys())[9999]\n",
    "# a_key.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(w2v[str.encode('owl')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "#TaggedDocument does not filter or stem\n",
    "\n",
    "# documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(short_sents)]\n",
    "# model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "\n",
    "# vector = model.infer_vector([\"system\", \"response\"])\n",
    "# vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order of ops:\n",
    "# just grammar features\n",
    "# just tfidf vectorizer\n",
    "# both\n",
    "\n",
    "# kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EAP', 'EAP', 'EAP', ..., 'EAP', 'EAP', 'EAP'], dtype=object)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear svc\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lin_clf = LinearSVC()\n",
    "lin_clf.fit(X_train, Y_train) \n",
    "preds = lin_clf.predict(X_val)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4964249233912155\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "\n",
    "accuracy = Eval.get_accuracy(preds, Y_val)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Extract\n",
    "import Utils\n",
    "importlib.reload(Extract)\n",
    "importlib.reload(Utils)\n",
    "from Utils import Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploration\n",
    "\n",
    "\n",
    "mws_df = train_df[train_df.author == 'MWS']\n",
    "hpl_df = train_df[train_df.author == 'HPL']\n",
    "eap_df = train_df[train_df.author == 'EAP']\n",
    "\n",
    "cutoff = min([mws_df.shape[0], hpl_df.shape[0], eap_df.shape[0]])\n",
    "\n",
    "# equalize corpus sizes to avoid bias during exploration\n",
    "mws_df = mws_df[:cutoff]\n",
    "hpl_df = hpl_df[:cutoff]\n",
    "eap_df = eap_df[:cutoff]\n",
    "\n",
    "mws_lexicon = Utils.build_lexicon(mws_df.text, STOPWORDS)\n",
    "hpl_lexicon = Utils.build_lexicon(hpl_df.text, STOPWORDS)\n",
    "eap_lexicon = Utils.build_lexicon(eap_df.text, STOPWORDS)\n",
    "\n",
    "# sanity check\n",
    "assert(cutoff * 3 == len(mws_df) + len(hpl_df) + len(eap_df))\n",
    "\n",
    "# add grammatical features (for exploration this time, not training)\n",
    "mws_gram_feats_df = Extract.gram_feats(mws_df.text, None, None)\n",
    "hpl_gram_feats_df = Extract.gram_feats(hpl_df.text, None, None)\n",
    "eap_gram_feats_df = Extract.gram_feats(eap_df.text, None, None)\n",
    "\n",
    "mws_gram_feats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like sentence length values are consistently higher by at least a degree of magnitude\n",
    "# so we'll take the log\n",
    "for df in [mws_gram_feats_df, hpl_gram_feats_df, eap_gram_feats_df]:\n",
    "    df['sent_len'] = df['sent_len'].apply(lambda x: math.log(x))\n",
    "    df.rename(inplace=True, columns={'sent_len': 'log_sent_len'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example MWS sentence: \n",
      "3    How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.\n",
      "\n",
      "Example HPL sentence: \n",
      "1    It never once occurred to me that the fumbling might be a mere mistake.\n",
      "\n",
      "Example EAP sentence: \n",
      "0    This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data viz\n",
    "\n",
    "def plot_word_freq(lexicon, name, quantity=20):\n",
    "    plt.rcdefaults()\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    elems = [x[0] for x in lexicon[:quantity]]\n",
    "    y_pos = np.arange(quantity)\n",
    "    vals = [x[1] for x in lexicon[:quantity]]\n",
    "\n",
    "    ax.barh(y_pos, vals, align='center',\n",
    "            color='green', ecolor='black')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(elems)\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_xlabel('Corpus-wide frequency')\n",
    "    ax.set_title(name + ' - Word Frequencies')\n",
    "\n",
    "    plt.show()\n",
    " \n",
    "authors = {'MWS': mws_lexicon, 'HPL': hpl_lexicon, 'EAP': eap_lexicon}\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(\"Example MWS sentence: \\n{}\\n\".format(mws_df.text[:1].to_string()))\n",
    "print(\"Example HPL sentence: \\n{}\\n\".format(hpl_df.text[:1].to_string()))\n",
    "print(\"Example EAP sentence: \\n{}\\n\".format(eap_df.text[:1].to_string()))\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "\n",
    "# for key in authors: \n",
    "#     plot_word_freq(authors[key], key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mws_gram_feats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mws_gram_feats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 4))\n",
    "\n",
    "\n",
    "\n",
    "# # generate some random test data\n",
    "# all_data = [np.random.normal(0, std, 100) for std in range(6, 10)]\n",
    "\n",
    "# # plot violin plot\n",
    "# axes[0].violinplot(all_data,\n",
    "#                    showmeans=False,\n",
    "#                    showmedians=True)\n",
    "# axes[0].set_title('Violin plot')\n",
    "\n",
    "# # plot box plot\n",
    "# axes[1].boxplot(all_data)\n",
    "# axes[1].set_title('Box plot')\n",
    "\n",
    "# # adding horizontal grid lines\n",
    "# for ax in axes:\n",
    "#     ax.yaxis.grid(True)\n",
    "#     ax.set_xticks([y + 1 for y in range(len(all_data))])\n",
    "#     ax.set_xlabel('Four separate samples')\n",
    "#     ax.set_ylabel('Observed values')\n",
    "\n",
    "# # add x-tick labels\n",
    "# plt.setp(axes, xticks=[y + 1 for y in range(len(all_data))],\n",
    "#          xticklabels=list(mws_gram_feats_df.columns))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(df, subset):\n",
    "    features = {\n",
    "        'tag_features': [\n",
    "             'ADJ_count',\n",
    "             'ADP_count',\n",
    "             'ADV_count',\n",
    "             'CCONJ_count',\n",
    "             'DET_count',\n",
    "             'NOUN_count',\n",
    "             'PRON_count',\n",
    "             'VERB_count'],\n",
    "        'punc_features': [\n",
    "            'bang_count',\n",
    "            'colon_count',\n",
    "            'ellipse_count',\n",
    "            'lparen_count',\n",
    "            'quote_count',\n",
    "            'semicolon_count'],\n",
    "        'ratio_features': [\n",
    "             'adj_noun_ratio',\n",
    "             'adv_verb_ratio',\n",
    "             'log_sent_len']\n",
    "    }\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    boxplot = df.boxplot(column=features[subset], \\\n",
    "        showfliers=False, fontsize=6, figsize=None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(mws_gram_feats_df, 'ratio_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(hpl_gram_feats_df, 'ratio_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(eap_gram_feats_df, 'ratio_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(mws_gram_feats_df, 'tag_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(hpl_gram_feats_df, 'tag_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(eap_gram_feats_df, 'tag_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(mws_gram_feats_df, 'punc_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(hpl_gram_feats_df, 'punc_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(eap_gram_feats_df, 'punc_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJ_count</th>\n",
       "      <th>ADP_count</th>\n",
       "      <th>ADV_count</th>\n",
       "      <th>CCONJ_count</th>\n",
       "      <th>DET_count</th>\n",
       "      <th>INTJ_count</th>\n",
       "      <th>NOUN_count</th>\n",
       "      <th>NUM_count</th>\n",
       "      <th>PART_count</th>\n",
       "      <th>PRON_count</th>\n",
       "      <th>...</th>\n",
       "      <th>X_count</th>\n",
       "      <th>adj_noun_ratio</th>\n",
       "      <th>adv_verb_ratio</th>\n",
       "      <th>bang_count</th>\n",
       "      <th>colon_count</th>\n",
       "      <th>ellipse_count</th>\n",
       "      <th>lparen_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>semicolon_count</th>\n",
       "      <th>log_sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5018.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>3307.000000</td>\n",
       "      <td>5004.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>1949.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.918012</td>\n",
       "      <td>4.378836</td>\n",
       "      <td>1.936646</td>\n",
       "      <td>1.728152</td>\n",
       "      <td>3.605516</td>\n",
       "      <td>1.439306</td>\n",
       "      <td>5.114286</td>\n",
       "      <td>1.504624</td>\n",
       "      <td>1.420729</td>\n",
       "      <td>2.127696</td>\n",
       "      <td>...</td>\n",
       "      <td>1.989130</td>\n",
       "      <td>1.692336</td>\n",
       "      <td>1.787072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022538</td>\n",
       "      <td>0.011180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.165217</td>\n",
       "      <td>0.172671</td>\n",
       "      <td>0.425099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.850061</td>\n",
       "      <td>3.656925</td>\n",
       "      <td>2.000106</td>\n",
       "      <td>1.218010</td>\n",
       "      <td>2.864495</td>\n",
       "      <td>0.995959</td>\n",
       "      <td>4.344305</td>\n",
       "      <td>1.394070</td>\n",
       "      <td>0.765330</td>\n",
       "      <td>1.478494</td>\n",
       "      <td>...</td>\n",
       "      <td>1.558349</td>\n",
       "      <td>1.518619</td>\n",
       "      <td>1.810267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157714</td>\n",
       "      <td>0.361998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.790786</td>\n",
       "      <td>0.523666</td>\n",
       "      <td>0.104491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.442810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.689468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ADJ_count    ADP_count    ADV_count  CCONJ_count    DET_count  \\\n",
       "count  5635.000000  5018.000000  5635.000000  3307.000000  5004.000000   \n",
       "mean      2.918012     4.378836     1.936646     1.728152     3.605516   \n",
       "std       2.850061     3.656925     2.000106     1.218010     2.864495   \n",
       "min       0.000000     1.000000     0.000000     1.000000     1.000000   \n",
       "25%       1.000000     2.000000     1.000000     1.000000     2.000000   \n",
       "50%       2.000000     3.000000     1.000000     1.000000     3.000000   \n",
       "75%       4.000000     6.000000     3.000000     2.000000     5.000000   \n",
       "max      39.000000    64.000000    19.000000    13.000000    45.000000   \n",
       "\n",
       "       INTJ_count   NOUN_count   NUM_count   PART_count   PRON_count  \\\n",
       "count  173.000000  5635.000000  757.000000  1949.000000  4080.000000   \n",
       "mean     1.439306     5.114286    1.504624     1.420729     2.127696   \n",
       "std      0.995959     4.344305    1.394070     0.765330     1.478494   \n",
       "min      1.000000     0.000000    1.000000     1.000000     1.000000   \n",
       "25%      1.000000     2.000000    1.000000     1.000000     1.000000   \n",
       "50%      1.000000     4.000000    1.000000     1.000000     2.000000   \n",
       "75%      2.000000     7.000000    2.000000     2.000000     3.000000   \n",
       "max      8.000000    67.000000   27.000000     8.000000    15.000000   \n",
       "\n",
       "           ...         X_count  adj_noun_ratio  adv_verb_ratio  bang_count  \\\n",
       "count      ...       92.000000     5635.000000     5635.000000      5635.0   \n",
       "mean       ...        1.989130        1.692336        1.787072         0.0   \n",
       "std        ...        1.558349        1.518619        1.810267         0.0   \n",
       "min        ...        1.000000        0.000000        0.000000         0.0   \n",
       "25%        ...        1.000000        0.800000        0.000000         0.0   \n",
       "50%        ...        1.000000        1.400000        1.500000         0.0   \n",
       "75%        ...        2.000000        2.125000        2.500000         0.0   \n",
       "max        ...        9.000000       14.000000       13.000000         0.0   \n",
       "\n",
       "       colon_count  ellipse_count  lparen_count  quote_count  semicolon_count  \\\n",
       "count  5635.000000    5635.000000        5635.0  5635.000000      5635.000000   \n",
       "mean      0.022538       0.011180           0.0     0.165217         0.172671   \n",
       "std       0.157714       0.361998           0.0     0.790786         0.523666   \n",
       "min       0.000000       0.000000           0.0     0.000000         0.000000   \n",
       "25%       0.000000       0.000000           0.0     0.000000         0.000000   \n",
       "50%       0.000000       0.000000           0.0     0.000000         0.000000   \n",
       "75%       0.000000       0.000000           0.0     0.000000         0.000000   \n",
       "max       3.000000      17.000000           0.0    23.000000        11.000000   \n",
       "\n",
       "       log_sent_len  \n",
       "count   5635.000000  \n",
       "mean       0.425099  \n",
       "std        0.104491  \n",
       "min        0.107368  \n",
       "25%        0.362002  \n",
       "50%        0.442810  \n",
       "75%        0.502329  \n",
       "max        0.689468  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eap_gram_feats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strangely enough, grepping through the raw input indeed shows that no bang characters exist\n",
    "# the boxplots indicate that the grammatical features indeed don't seem to have much \n",
    "# predictive power, so we'll try other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timenow = time.asctime( time.localtime(time.time()) )\n",
    "print(\"Finished at: \", timenow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
