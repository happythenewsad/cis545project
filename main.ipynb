{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# local code\n",
    "sys.path.insert(1, \"./code/\")\n",
    "from Utils import Utils # student's library\n",
    "from Eval import Eval # student's library\n",
    "from Extract import Extract # student's library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "#train_df.id.nunique()\n",
    "# 19579\n",
    "\n",
    "#train_df.author.unique()\n",
    "# array(['EAP', 'HPL', 'MWS'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 0, 'id': 0, 'text': 0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Utils.check_for_nulls(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data\n",
    "\n",
    "# regular data\n",
    "#     train: 19580 * .9 rows\n",
    "#     test:  8393 rows\n",
    "#     val:   19580 * .1 rows\n",
    "\n",
    "\n",
    "if os.path.isfile('data/traindata.pickle'):\n",
    "    traindata = pd.read_pickle('data/traindata.pickle')\n",
    "    valdata   = pd.read_pickle('data/valdata.pickle')\n",
    "    testdata  = pd.read_pickle('data/testdata.pickle')\n",
    "else: \n",
    "    VAL_IDX  = math.ceil(len(train_df) * .8)\n",
    "    TEST_IDX = math.ceil(len(train_df) * .9)\n",
    "\n",
    "    traindata = train_df[:VAL_IDX]\n",
    "    valdata   = train_df[VAL_IDX:TEST_IDX]\n",
    "    testdata  = train_df[TEST_IDX:]\n",
    "\n",
    "    print(VAL_IDX, TEST_IDX)\n",
    "\n",
    "    traindata.to_pickle('data/traindata.pickle')\n",
    "    valdata.to_pickle('data/valdata.pickle')\n",
    "    testdata.to_pickle('data/testdata.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindata: 15664, valdata: 1958, testdata: 1957\n"
     ]
    }
   ],
   "source": [
    "print(\"traindata: {}, valdata: {}, testdata: {}\".format(len(traindata), len(valdata), len(testdata)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at:  Mon Dec 10 01:50:52 2018\n"
     ]
    }
   ],
   "source": [
    "def logtime():\n",
    "    timenow = time.asctime( time.localtime(time.time()) )\n",
    "    print(\"Finished at: \", timenow)\n",
    "logtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "Y_train = list(traindata.author )\n",
    "Y_val = list(valdata.author)\n",
    "Y_test = list(testdata.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading gram feats from pickle\n",
      "Finished at:  Mon Dec 10 01:50:53 2018\n"
     ]
    }
   ],
   "source": [
    "# grammatical feature engineering \n",
    "# we want to include stopwords here\n",
    "\n",
    "def gen_gram_feats(name, seq_no, train, val, test):\n",
    "    train_df = Extract.gram_feats(train.text, None, seq_no)\n",
    "\n",
    "    # need to remember so that val/test process\n",
    "    # does not add additional columns\n",
    "    gram_feat_list = list(train_df.columns)\n",
    "\n",
    "    val_df = Extract.gram_feats(val.text, gram_feat_list, seq_no)\n",
    "    test_df = Extract.gram_feats(test.text, gram_feat_list, seq_no)\n",
    "\n",
    "    # removes a singleton feature\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        if 'SYM_count' in list(df.columns):\n",
    "            df.drop('SYM_count', axis=1, inplace=True)\n",
    "        \n",
    "    train_df.to_pickle('data/train_' + name + '_df.pickle')\n",
    "    val_df.to_pickle('data/val_' + name + '_df.pickle')\n",
    "    test_df.to_pickle('data/test_' + name + '_df.pickle')       \n",
    "\n",
    "\n",
    "if os.path.isfile('data/train_gram_df.pickle'):\n",
    "    print(\"reading gram feats from pickle\")\n",
    "    # pass\n",
    "else:\n",
    "    print(\"writing gram feats pickles\")\n",
    "    gen_gram_feats('gram', None, traindata, valdata, testdata)\n",
    "    gen_gram_feats('gram_seq', 7, traindata, valdata, testdata)\n",
    "    \n",
    "train_gram_df = pd.read_pickle('data/train_gram_df.pickle')\n",
    "val_gram_df   = pd.read_pickle('data/val_gram_df.pickle')\n",
    "test_gram_df  = pd.read_pickle('data/test_gram_df.pickle')\n",
    "\n",
    "train_gram_seq_df = pd.read_pickle('data/train_gram_seq_df.pickle')\n",
    "val_gram_seq_df   = pd.read_pickle('data/val_gram_seq_df.pickle')\n",
    "test_gram_seq_df  = pd.read_pickle('data/test_gram_seq_df.pickle')\n",
    "    \n",
    "logtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15664, 23)\n",
      "(1958, 23)\n",
      "(1957, 23)\n",
      "(15664, 1622)\n",
      "(1958, 1622)\n",
      "(1957, 1622)\n"
     ]
    }
   ],
   "source": [
    "print(train_gram_df.shape)\n",
    "print(val_gram_df.shape)\n",
    "print(test_gram_df.shape)\n",
    "\n",
    "print(train_gram_seq_df.shape)\n",
    "print(val_gram_seq_df.shape)\n",
    "print(test_gram_seq_df.shape)\n",
    "\n",
    "assert(train_gram_df.shape == (15664, 23))\n",
    "assert(train_gram_seq_df.shape == (15664, 1622))\n",
    "#set(GRAM_FEAT_LIST) - set(list(val_gram_feats_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ADJ_count</th>\n",
       "      <th>ADP_count</th>\n",
       "      <th>ADV_count</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>CCONJ_CCONJ_CCONJ_VERB_NOUN</th>\n",
       "      <th>CCONJ_CCONJ_NOUN</th>\n",
       "      <th>CCONJ_CCONJ_NOUN_CCONJ_NOUN_CCONJ_NOUN</th>\n",
       "      <th>CCONJ_CCONJ_NOUN_CCONJ_NOUN_NOUN_VERB</th>\n",
       "      <th>CCONJ_CCONJ_NOUN_CCONJ_NOUN_VERB_VERB</th>\n",
       "      <th>...</th>\n",
       "      <th>X_count</th>\n",
       "      <th>adj_noun_ratio</th>\n",
       "      <th>adv_verb_ratio</th>\n",
       "      <th>bang_count</th>\n",
       "      <th>colon_count</th>\n",
       "      <th>ellipse_count</th>\n",
       "      <th>lparen_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>semicolon_count</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235080</td>\n",
       "      <td>0.513819</td>\n",
       "      <td>0.627603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363031</td>\n",
       "      <td>0.277382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1611</td>\n",
       "      <td>-0.018801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.237475</td>\n",
       "      <td>2.918125</td>\n",
       "      <td>0.762854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.737871</td>\n",
       "      <td>-0.660999</td>\n",
       "      <td>0.084256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.191755</td>\n",
       "      <td>-0.288286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1611</td>\n",
       "      <td>-0.018801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.237475</td>\n",
       "      <td>-0.440646</td>\n",
       "      <td>-0.726940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.208031</td>\n",
       "      <td>0.513819</td>\n",
       "      <td>-0.459090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.199733</td>\n",
       "      <td>0.923860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1611</td>\n",
       "      <td>-0.018801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.237475</td>\n",
       "      <td>-0.440646</td>\n",
       "      <td>0.474207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.883714</td>\n",
       "      <td>0.513819</td>\n",
       "      <td>0.084256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.493349</td>\n",
       "      <td>0.439002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1611</td>\n",
       "      <td>-0.018801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.237475</td>\n",
       "      <td>-0.440646</td>\n",
       "      <td>0.530074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235080</td>\n",
       "      <td>-0.367294</td>\n",
       "      <td>1.170950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.150797</td>\n",
       "      <td>-0.409501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1611</td>\n",
       "      <td>-0.018801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.237475</td>\n",
       "      <td>1.238739</td>\n",
       "      <td>0.232115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1622 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ADJ_count  ADP_count  ADV_count  CCONJ  CCONJ_CCONJ_CCONJ_VERB_NOUN  \\\n",
       "0  0.0   0.235080   0.513819   0.627603    0.0                          0.0   \n",
       "1  0.0  -0.737871  -0.660999   0.084256    0.0                          0.0   \n",
       "2  0.0   1.208031   0.513819  -0.459090    0.0                          0.0   \n",
       "3  0.0   0.883714   0.513819   0.084256    0.0                          0.0   \n",
       "4  0.0   0.235080  -0.367294   1.170950    0.0                          0.0   \n",
       "\n",
       "   CCONJ_CCONJ_NOUN  CCONJ_CCONJ_NOUN_CCONJ_NOUN_CCONJ_NOUN  \\\n",
       "0               0.0                                     0.0   \n",
       "1               0.0                                     0.0   \n",
       "2               0.0                                     0.0   \n",
       "3               0.0                                     0.0   \n",
       "4               0.0                                     0.0   \n",
       "\n",
       "   CCONJ_CCONJ_NOUN_CCONJ_NOUN_NOUN_VERB  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   CCONJ_CCONJ_NOUN_CCONJ_NOUN_VERB_VERB    ...     X_count  adj_noun_ratio  \\\n",
       "0                                    0.0    ...         0.0        0.363031   \n",
       "1                                    0.0    ...         0.0        0.191755   \n",
       "2                                    0.0    ...         0.0       -0.199733   \n",
       "3                                    0.0    ...         0.0       -0.493349   \n",
       "4                                    0.0    ...         0.0       -0.150797   \n",
       "\n",
       "   adv_verb_ratio  bang_count  colon_count  ellipse_count  lparen_count  \\\n",
       "0        0.277382         0.0      -0.1611      -0.018801           0.0   \n",
       "1       -0.288286         0.0      -0.1611      -0.018801           0.0   \n",
       "2        0.923860         0.0      -0.1611      -0.018801           0.0   \n",
       "3        0.439002         0.0      -0.1611      -0.018801           0.0   \n",
       "4       -0.409501         0.0      -0.1611      -0.018801           0.0   \n",
       "\n",
       "   quote_count  semicolon_count  sent_len  \n",
       "0    -0.237475         2.918125  0.762854  \n",
       "1    -0.237475        -0.440646 -0.726940  \n",
       "2    -0.237475        -0.440646  0.474207  \n",
       "3    -0.237475        -0.440646  0.530074  \n",
       "4    -0.237475         1.238739  0.232115  \n",
       "\n",
       "[5 rows x 1622 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gram_seq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15664, 1500)\n",
      "(1958, 1500)\n",
      "(1957, 1500)\n"
     ]
    }
   ],
   "source": [
    "# textual feature engineering\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=STOPWORDS, max_features=1500)\n",
    "\n",
    "train_text_feats = vectorizer.fit_transform(traindata.text)\n",
    "val_text_feats = vectorizer.transform(valdata.text) \n",
    "test_text_feats = vectorizer.transform(testdata.text) \n",
    "\n",
    "cols = [\"text_\" + str(x) for x in range(train_text_feats.shape[1])]\n",
    "\n",
    "train_text_feats_df = pd.DataFrame(train_text_feats.todense(), index=None, columns=cols)\n",
    "val_text_feats_df = pd.DataFrame(val_text_feats.todense(), index=None, columns=cols)\n",
    "test_text_feats_df = pd.DataFrame(test_text_feats.todense(), index=None, columns=cols)\n",
    "\n",
    "print(train_text_feats_df.shape)\n",
    "print(val_text_feats_df.shape)\n",
    "print(test_text_feats_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist to disk\n",
    "# if not os.path.isfile('data/train_text_feats_df.pickle'):\n",
    "#     train_text_feats_df.to_pickle('data/train_text_feats_df.pickle')\n",
    "#     val_text_feats_df.to_pickle('data/val_text_feats_df.pickle')\n",
    "#     test_text_feats_df.to_pickle('data/test_text_feats_df.pickle')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_0</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>text_3</th>\n",
       "      <th>text_4</th>\n",
       "      <th>text_5</th>\n",
       "      <th>text_6</th>\n",
       "      <th>text_7</th>\n",
       "      <th>text_8</th>\n",
       "      <th>text_9</th>\n",
       "      <th>...</th>\n",
       "      <th>text_1490</th>\n",
       "      <th>text_1491</th>\n",
       "      <th>text_1492</th>\n",
       "      <th>text_1493</th>\n",
       "      <th>text_1494</th>\n",
       "      <th>text_1495</th>\n",
       "      <th>text_1496</th>\n",
       "      <th>text_1497</th>\n",
       "      <th>text_1498</th>\n",
       "      <th>text_1499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_0  text_1  text_2  text_3  text_4  text_5  text_6  text_7  text_8  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   text_9    ...      text_1490  text_1491  text_1492  text_1493  text_1494  \\\n",
       "0     0.0    ...            0.0        0.0        0.0        0.0   0.000000   \n",
       "1     0.0    ...            0.0        0.0        0.0        0.0   0.000000   \n",
       "2     0.0    ...            0.0        0.0        0.0        0.0   0.000000   \n",
       "3     0.0    ...            0.0        0.0        0.0        0.0   0.225799   \n",
       "4     0.0    ...            0.0        0.0        0.0        0.0   0.000000   \n",
       "\n",
       "   text_1495  text_1496  text_1497  text_1498  text_1499  \n",
       "0        0.0        0.0        0.0        0.0        0.0  \n",
       "1        0.0        0.0        0.0        0.0        0.0  \n",
       "2        0.0        0.0        0.0        0.0        0.0  \n",
       "3        0.0        0.0        0.0        0.0        0.0  \n",
       "4        0.0        0.0        0.0        0.0        0.0  \n",
       "\n",
       "[5 rows x 1500 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_feats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at:  Mon Dec 10 01:50:54 2018\n",
      "Finished at:  Mon Dec 10 01:51:19 2018\n"
     ]
    }
   ],
   "source": [
    "# gensim feature engineering\n",
    "import numpy as np\n",
    "import gensim\n",
    "GENSIM = True\n",
    "\n",
    "if GENSIM:\n",
    "    logtime()\n",
    "    #https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "    from gensim.test.utils import common_texts\n",
    "    from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "    #TaggedDocument does not filter or stem\n",
    "\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(list(traindata.text))]\n",
    "    model = Doc2Vec(documents, vector_size=1500, window=2, min_count=1, workers=4)\n",
    "\n",
    "    train_gensim = np.array([model.infer_vector(x) for x in list(traindata.text)])\n",
    "    val_gensim = np.array([model.infer_vector(x) for x in list(valdata.text)])\n",
    "    test_gensim = np.array([model.infer_vector(x) for x in list(testdata.text)])\n",
    "\n",
    "    # numpy to pandas\n",
    "    cols = [\"gensim_\" + str(x) for x in range(len(train_gensim[0]))]\n",
    "\n",
    "    train_gensim_df = pd.DataFrame(train_gensim, index=None, columns=cols)\n",
    "    val_gensim_df = pd.DataFrame(val_gensim, index=None, columns=cols)\n",
    "    test_gensim_df = pd.DataFrame(test_gensim, index=None, columns=cols)\n",
    "    logtime()\n",
    "    \n",
    "# gensim didn't help. so we're settling on tfidf extual features for now, and will explore neural models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lex = Utils.build_lexicon(traindata.text, STOPWORDS)\n",
    "# len(lex)\n",
    "\n",
    "# 22847 different tokens in full lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJ_count</th>\n",
       "      <th>ADP_count</th>\n",
       "      <th>ADV_count</th>\n",
       "      <th>CCONJ_count</th>\n",
       "      <th>DET_count</th>\n",
       "      <th>INTJ_count</th>\n",
       "      <th>NOUN_count</th>\n",
       "      <th>NUM_count</th>\n",
       "      <th>PART_count</th>\n",
       "      <th>PRON_count</th>\n",
       "      <th>...</th>\n",
       "      <th>X_count</th>\n",
       "      <th>adj_noun_ratio</th>\n",
       "      <th>adv_verb_ratio</th>\n",
       "      <th>bang_count</th>\n",
       "      <th>colon_count</th>\n",
       "      <th>ellipse_count</th>\n",
       "      <th>lparen_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>semicolon_count</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.043782e-18</td>\n",
       "      <td>6.777582e-17</td>\n",
       "      <td>2.395796e-17</td>\n",
       "      <td>-7.187389e-17</td>\n",
       "      <td>9.457091e-18</td>\n",
       "      <td>2.521891e-18</td>\n",
       "      <td>4.980734e-17</td>\n",
       "      <td>1.670753e-17</td>\n",
       "      <td>1.891418e-18</td>\n",
       "      <td>4.224167e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>7.880909e-20</td>\n",
       "      <td>1.387040e-17</td>\n",
       "      <td>4.539403e-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.120840e-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.639229e-17</td>\n",
       "      <td>-7.565672e-18</td>\n",
       "      <td>-3.152364e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.609892e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.428109e-01</td>\n",
       "      <td>9.289644e-01</td>\n",
       "      <td>1.797328e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.101639e-01</td>\n",
       "      <td>6.326520e-01</td>\n",
       "      <td>9.163641e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>5.807221e-02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-9.669821e-01</td>\n",
       "      <td>-8.362820e-01</td>\n",
       "      <td>-8.746468e-01</td>\n",
       "      <td>-6.749403e-01</td>\n",
       "      <td>-7.230532e-01</td>\n",
       "      <td>-2.697985e-01</td>\n",
       "      <td>-1.032234e+00</td>\n",
       "      <td>-3.935250e-01</td>\n",
       "      <td>-5.018364e-01</td>\n",
       "      <td>-6.978646e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.874173e-01</td>\n",
       "      <td>-1.238327e+00</td>\n",
       "      <td>-9.793490e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.231806e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.097765e-01</td>\n",
       "      <td>-5.894256e-01</td>\n",
       "      <td>-1.017688e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.949294e-01</td>\n",
       "      <td>-5.730821e-01</td>\n",
       "      <td>-8.746468e-01</td>\n",
       "      <td>-6.749403e-01</td>\n",
       "      <td>-3.758420e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-4.800391e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-5.018364e-01</td>\n",
       "      <td>-6.978646e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-4.909876e-01</td>\n",
       "      <td>-9.793490e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.231806e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.097765e-01</td>\n",
       "      <td>-5.894256e-01</td>\n",
       "      <td>-5.281714e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.508239e-01</td>\n",
       "      <td>-4.668209e-02</td>\n",
       "      <td>-3.236535e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.863082e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.119090e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.920516e-01</td>\n",
       "      <td>-1.519656e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.231806e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.097765e-01</td>\n",
       "      <td>-5.894256e-01</td>\n",
       "      <td>-1.785166e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.932816e-01</td>\n",
       "      <td>2.165179e-01</td>\n",
       "      <td>2.273397e-01</td>\n",
       "      <td>9.130644e-02</td>\n",
       "      <td>3.185804e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.562212e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000804e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.563523e-01</td>\n",
       "      <td>4.685719e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.231806e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.097765e-01</td>\n",
       "      <td>7.428701e-01</td>\n",
       "      <td>3.110000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.059114e+01</td>\n",
       "      <td>3.311652e+01</td>\n",
       "      <td>2.171608e+01</td>\n",
       "      <td>1.924748e+01</td>\n",
       "      <td>3.226201e+01</td>\n",
       "      <td>7.693608e+00</td>\n",
       "      <td>3.449232e+01</td>\n",
       "      <td>7.238050e+00</td>\n",
       "      <td>1.794448e+01</td>\n",
       "      <td>2.443740e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.023842e+00</td>\n",
       "      <td>9.224431e+00</td>\n",
       "      <td>6.467101e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.138564e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.594267e+01</td>\n",
       "      <td>7.404349e+00</td>\n",
       "      <td>3.505114e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ADJ_count     ADP_count     ADV_count   CCONJ_count     DET_count  \\\n",
       "count  5.635000e+03  5.635000e+03  5.635000e+03  5.635000e+03  5.635000e+03   \n",
       "mean   5.043782e-18  6.777582e-17  2.395796e-17 -7.187389e-17  9.457091e-18   \n",
       "std    1.000000e+00  9.609892e-01  1.000000e+00  8.428109e-01  9.289644e-01   \n",
       "min   -9.669821e-01 -8.362820e-01 -8.746468e-01 -6.749403e-01 -7.230532e-01   \n",
       "25%   -6.949294e-01 -5.730821e-01 -8.746468e-01 -6.749403e-01 -3.758420e-01   \n",
       "50%   -1.508239e-01 -4.668209e-02 -3.236535e-01  0.000000e+00 -2.863082e-02   \n",
       "75%    3.932816e-01  2.165179e-01  2.273397e-01  9.130644e-02  3.185804e-01   \n",
       "max    3.059114e+01  3.311652e+01  2.171608e+01  1.924748e+01  3.226201e+01   \n",
       "\n",
       "         INTJ_count    NOUN_count     NUM_count    PART_count    PRON_count  \\\n",
       "count  5.635000e+03  5.635000e+03  5.635000e+03  5.635000e+03  5.635000e+03   \n",
       "mean   2.521891e-18  4.980734e-17  1.670753e-17  1.891418e-18  4.224167e-17   \n",
       "std    1.797328e-01  1.000000e+00  3.101639e-01  6.326520e-01  9.163641e-01   \n",
       "min   -2.697985e-01 -1.032234e+00 -3.935250e-01 -5.018364e-01 -6.978646e-01   \n",
       "25%    0.000000e+00 -4.800391e-01  0.000000e+00 -5.018364e-01 -6.978646e-01   \n",
       "50%    0.000000e+00 -1.119090e-01  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  2.562212e-01  0.000000e+00  0.000000e+00  1.000804e-01   \n",
       "max    7.693608e+00  3.449232e+01  7.238050e+00  1.794448e+01  2.443740e+01   \n",
       "\n",
       "           ...            X_count  adj_noun_ratio  adv_verb_ratio  bang_count  \\\n",
       "count      ...       5.635000e+03    5.635000e+03    5.635000e+03      5635.0   \n",
       "mean       ...       7.880909e-20    1.387040e-17    4.539403e-17         0.0   \n",
       "std        ...       5.807221e-02    1.000000e+00    1.000000e+00         0.0   \n",
       "min        ...      -2.874173e-01   -1.238327e+00   -9.793490e-01         0.0   \n",
       "25%        ...       0.000000e+00   -4.909876e-01   -9.793490e-01         0.0   \n",
       "50%        ...       0.000000e+00   -1.920516e-01   -1.519656e-01         0.0   \n",
       "75%        ...       0.000000e+00    2.563523e-01    4.685719e-01         0.0   \n",
       "max        ...       4.023842e+00    9.224431e+00    6.467101e+00         0.0   \n",
       "\n",
       "        colon_count  ellipse_count  lparen_count   quote_count  \\\n",
       "count  5.635000e+03         5635.0        5635.0  5.635000e+03   \n",
       "mean  -3.120840e-17            0.0           0.0  1.639229e-17   \n",
       "std    1.000000e+00            0.0           0.0  1.000000e+00   \n",
       "min   -2.231806e-01            0.0           0.0 -2.097765e-01   \n",
       "25%   -2.231806e-01            0.0           0.0 -2.097765e-01   \n",
       "50%   -2.231806e-01            0.0           0.0 -2.097765e-01   \n",
       "75%   -2.231806e-01            0.0           0.0 -2.097765e-01   \n",
       "max    1.138564e+01            0.0           0.0  2.594267e+01   \n",
       "\n",
       "       semicolon_count      sent_len  \n",
       "count     5.635000e+03  5.635000e+03  \n",
       "mean     -7.565672e-18 -3.152364e-17  \n",
       "std       1.000000e+00  1.000000e+00  \n",
       "min      -5.894256e-01 -1.017688e+00  \n",
       "25%      -5.894256e-01 -5.281714e-01  \n",
       "50%      -5.894256e-01 -1.785166e-01  \n",
       "75%       7.428701e-01  3.110000e-01  \n",
       "max       7.404349e+00  3.505114e+01  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mws_df = train_df[train_df.author == 'MWS']\n",
    "hpl_df = train_df[train_df.author == 'HPL']\n",
    "eap_df = train_df[train_df.author == 'EAP']\n",
    "\n",
    "cutoff = min([mws_df.shape[0], hpl_df.shape[0], eap_df.shape[0]])\n",
    "\n",
    "# equalize corpus sizes to avoid bias during exploration\n",
    "mws_df = mws_df[:cutoff]\n",
    "hpl_df = hpl_df[:cutoff]\n",
    "eap_df = eap_df[:cutoff]\n",
    "\n",
    "mws_lexicon = Utils.build_lexicon(mws_df.text, STOPWORDS)\n",
    "hpl_lexicon = Utils.build_lexicon(hpl_df.text, STOPWORDS)\n",
    "eap_lexicon = Utils.build_lexicon(eap_df.text, STOPWORDS)\n",
    "\n",
    "# sanity check\n",
    "assert(cutoff * 3 == len(mws_df) + len(hpl_df) + len(eap_df))\n",
    "\n",
    "# add grammatical features (for exploration this time, not training)\n",
    "mws_gram_feats_df = Extract.gram_feats(mws_df.text, None, None)\n",
    "hpl_gram_feats_df = Extract.gram_feats(hpl_df.text, None, None)\n",
    "eap_gram_feats_df = Extract.gram_feats(eap_df.text, None, None)\n",
    "\n",
    "mws_gram_feats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like sentence length values are consistently higher by at least a degree of magnitude\n",
    "# so we'll take the log\n",
    "# This is not done via the standardize() method\n",
    "\n",
    "# for df in [mws_gram_feats_df, hpl_gram_feats_df, eap_gram_feats_df]:\n",
    "#     df['sent_len'] = df['sent_len'].apply(lambda x: math.log(x))\n",
    "#     df.rename(inplace=True, columns={'sent_len': 'log_sent_len'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER exploration\n",
    "\n",
    "# avoiding NER for three reasons:\n",
    "# features very sparse\n",
    "# features seem content-specific, so may contribute to misprediction\n",
    "# should we add data from the same authors about other topics\n",
    "\n",
    "NER = False\n",
    "if NER:\n",
    "    import spacy\n",
    "    spacy_mdl = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def sent_to_ents(sent, spacy):\n",
    "        sent = spacy(sent)\n",
    "        ents = []\n",
    "        for ent in sent.ents:\n",
    "            ents.append(ent.text + ':' + ent.label_)\n",
    "        return ents\n",
    "\n",
    "    entities = []\n",
    "    for i in range(1500):\n",
    "        sent = valdata.iloc[i].text\n",
    "        ents = sent_to_ents(sent, spacy_mdl)\n",
    "        entities.append(ents)\n",
    " \n",
    "\n",
    "# example entities list HERE\n",
    "# many sentences don't have any entities, like below\n",
    "#'In whatever way the shifting is managed, it is of course concealed at every step from observation.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NER:\n",
    "    import statistics as stat\n",
    "    entity_freq = [len(x) for x in entities]\n",
    "\n",
    "\n",
    "    print(\"stats for NER within a sample group: \\n\")\n",
    "    print(\"min: {} \\nmax: {} \\nmean: {} \\nstdev: {}\" \\\n",
    "          .format(min(entity_freq), max(entity_freq), stat.mean(entity_freq), stat.stdev(entity_freq)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration - visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example MWS sentence: \n",
      "3    How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.\n",
      "\n",
      "Example HPL sentence: \n",
      "1    It never once occurred to me that the fumbling might be a mere mistake.\n",
      "\n",
      "Example EAP sentence: \n",
      "0    This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data viz\n",
    "\n",
    "def plot_word_freq(lexicon, name, quantity=20):\n",
    "    plt.rcdefaults()\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    elems = [x[0] for x in lexicon[:quantity]]\n",
    "    y_pos = np.arange(quantity)\n",
    "    vals = [x[1] for x in lexicon[:quantity]]\n",
    "\n",
    "    ax.barh(y_pos, vals, align='center',\n",
    "            color='green', ecolor='black')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(elems)\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_xlabel('Corpus-wide frequency')\n",
    "    ax.set_title(name + ' - Word Frequencies')\n",
    "\n",
    "    plt.show()\n",
    " \n",
    "authors = {'MWS': mws_lexicon, 'HPL': hpl_lexicon, 'EAP': eap_lexicon}\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(\"Example MWS sentence: \\n{}\\n\".format(mws_df.text[:1].to_string()))\n",
    "print(\"Example HPL sentence: \\n{}\\n\".format(hpl_df.text[:1].to_string()))\n",
    "print(\"Example EAP sentence: \\n{}\\n\".format(eap_df.text[:1].to_string()))\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "\n",
    "# for key in authors: \n",
    "#     plot_word_freq(authors[key], key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(df, subset):\n",
    "    features = {\n",
    "        'tag_features': [\n",
    "             'ADJ_count',\n",
    "             'ADP_count',\n",
    "             'ADV_count',\n",
    "             'CCONJ_count',\n",
    "             'DET_count',\n",
    "             'NOUN_count',\n",
    "             'PRON_count',\n",
    "             'VERB_count'],\n",
    "        'punc_features': [\n",
    "            'bang_count',\n",
    "            'colon_count',\n",
    "            'ellipse_count',\n",
    "            'lparen_count',\n",
    "            'quote_count',\n",
    "            'semicolon_count'],\n",
    "        'ratio_features': [\n",
    "             'adj_noun_ratio',\n",
    "             'adv_verb_ratio',\n",
    "             'log_sent_len']\n",
    "    }\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    boxplot = df.boxplot(column=features[subset], \\\n",
    "        showfliers=False, fontsize=6, figsize=None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(mws_gram_feats_df, 'ratio_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(hpl_gram_feats_df, 'ratio_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(eap_gram_feats_df, 'ratio_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(mws_gram_feats_df, 'tag_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(hpl_gram_feats_df, 'tag_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(eap_gram_feats_df, 'tag_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(mws_gram_feats_df, 'punc_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(hpl_gram_feats_df, 'punc_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(eap_gram_feats_df, 'punc_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJ_count</th>\n",
       "      <th>ADP_count</th>\n",
       "      <th>ADV_count</th>\n",
       "      <th>CCONJ_count</th>\n",
       "      <th>DET_count</th>\n",
       "      <th>INTJ_count</th>\n",
       "      <th>NOUN_count</th>\n",
       "      <th>NUM_count</th>\n",
       "      <th>PART_count</th>\n",
       "      <th>PRON_count</th>\n",
       "      <th>...</th>\n",
       "      <th>X_count</th>\n",
       "      <th>adj_noun_ratio</th>\n",
       "      <th>adv_verb_ratio</th>\n",
       "      <th>bang_count</th>\n",
       "      <th>colon_count</th>\n",
       "      <th>ellipse_count</th>\n",
       "      <th>lparen_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>semicolon_count</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.278458e-17</td>\n",
       "      <td>3.782836e-18</td>\n",
       "      <td>-3.908931e-17</td>\n",
       "      <td>4.413309e-17</td>\n",
       "      <td>9.330996e-17</td>\n",
       "      <td>-7.880909e-19</td>\n",
       "      <td>9.330996e-17</td>\n",
       "      <td>-1.008756e-17</td>\n",
       "      <td>-3.026269e-17</td>\n",
       "      <td>-1.052889e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.418564e-18</td>\n",
       "      <td>1.525744e-16</td>\n",
       "      <td>1.008756e-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.040280e-17</td>\n",
       "      <td>-3.152364e-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.809105e-17</td>\n",
       "      <td>-1.197898e-17</td>\n",
       "      <td>-4.287214e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.436558e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>7.660251e-01</td>\n",
       "      <td>9.423383e-01</td>\n",
       "      <td>1.747253e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.663131e-01</td>\n",
       "      <td>5.880118e-01</td>\n",
       "      <td>8.508802e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.270903e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.023842e+00</td>\n",
       "      <td>-9.239556e-01</td>\n",
       "      <td>-9.682718e-01</td>\n",
       "      <td>-5.978215e-01</td>\n",
       "      <td>-9.095897e-01</td>\n",
       "      <td>-4.410888e-01</td>\n",
       "      <td>-1.177239e+00</td>\n",
       "      <td>-3.619786e-01</td>\n",
       "      <td>-5.497347e-01</td>\n",
       "      <td>-7.627328e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.347297e-01</td>\n",
       "      <td>-1.114391e+00</td>\n",
       "      <td>-9.871867e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.429028e-01</td>\n",
       "      <td>-3.088445e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.089280e-01</td>\n",
       "      <td>-3.297344e-01</td>\n",
       "      <td>-1.122274e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.729724e-01</td>\n",
       "      <td>-6.505018e-01</td>\n",
       "      <td>-4.682982e-01</td>\n",
       "      <td>-5.978215e-01</td>\n",
       "      <td>-5.604881e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-7.168663e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-7.627328e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-5.875971e-01</td>\n",
       "      <td>-9.871867e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.429028e-01</td>\n",
       "      <td>-3.088445e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.089280e-01</td>\n",
       "      <td>-3.297344e-01</td>\n",
       "      <td>-6.953480e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-3.221027e-01</td>\n",
       "      <td>-1.035942e-01</td>\n",
       "      <td>-4.682982e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.113865e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.564934e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-8.636901e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.925014e-01</td>\n",
       "      <td>-1.585799e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.429028e-01</td>\n",
       "      <td>-3.088445e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.089280e-01</td>\n",
       "      <td>-3.297344e-01</td>\n",
       "      <td>-2.498604e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.796366e-01</td>\n",
       "      <td>1.698596e-01</td>\n",
       "      <td>5.316489e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.377151e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.340658e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.849059e-01</td>\n",
       "      <td>3.938247e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.429028e-01</td>\n",
       "      <td>-3.088445e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.089280e-01</td>\n",
       "      <td>-3.297344e-01</td>\n",
       "      <td>3.998090e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.266008e+01</td>\n",
       "      <td>1.630363e+01</td>\n",
       "      <td>8.531226e+00</td>\n",
       "      <td>9.254316e+00</td>\n",
       "      <td>1.445088e+01</td>\n",
       "      <td>6.587313e+00</td>\n",
       "      <td>1.424525e+01</td>\n",
       "      <td>1.828845e+01</td>\n",
       "      <td>8.596645e+00</td>\n",
       "      <td>8.706361e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.498908e+00</td>\n",
       "      <td>8.104509e+00</td>\n",
       "      <td>6.194072e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.887892e+01</td>\n",
       "      <td>4.693064e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.887605e+01</td>\n",
       "      <td>2.067601e+01</td>\n",
       "      <td>1.291059e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ADJ_count     ADP_count     ADV_count   CCONJ_count     DET_count  \\\n",
       "count  5.635000e+03  5.635000e+03  5.635000e+03  5.635000e+03  5.635000e+03   \n",
       "mean  -3.278458e-17  3.782836e-18 -3.908931e-17  4.413309e-17  9.330996e-17   \n",
       "std    1.000000e+00  9.436558e-01  1.000000e+00  7.660251e-01  9.423383e-01   \n",
       "min   -1.023842e+00 -9.239556e-01 -9.682718e-01 -5.978215e-01 -9.095897e-01   \n",
       "25%   -6.729724e-01 -6.505018e-01 -4.682982e-01 -5.978215e-01 -5.604881e-01   \n",
       "50%   -3.221027e-01 -1.035942e-01 -4.682982e-01  0.000000e+00 -2.113865e-01   \n",
       "75%    3.796366e-01  1.698596e-01  5.316489e-01  0.000000e+00  1.377151e-01   \n",
       "max    1.266008e+01  1.630363e+01  8.531226e+00  9.254316e+00  1.445088e+01   \n",
       "\n",
       "         INTJ_count    NOUN_count     NUM_count    PART_count    PRON_count  \\\n",
       "count  5.635000e+03  5.635000e+03  5.635000e+03  5.635000e+03  5.635000e+03   \n",
       "mean  -7.880909e-19  9.330996e-17 -1.008756e-17 -3.026269e-17 -1.052889e-16   \n",
       "std    1.747253e-01  1.000000e+00  3.663131e-01  5.880118e-01  8.508802e-01   \n",
       "min   -4.410888e-01 -1.177239e+00 -3.619786e-01 -5.497347e-01 -7.627328e-01   \n",
       "25%    0.000000e+00 -7.168663e-01  0.000000e+00  0.000000e+00 -7.627328e-01   \n",
       "50%    0.000000e+00 -2.564934e-01  0.000000e+00  0.000000e+00 -8.636901e-02   \n",
       "75%    0.000000e+00  4.340658e-01  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    6.587313e+00  1.424525e+01  1.828845e+01  8.596645e+00  8.706361e+00   \n",
       "\n",
       "           ...            X_count  adj_noun_ratio  adv_verb_ratio  bang_count  \\\n",
       "count      ...       5.635000e+03    5.635000e+03    5.635000e+03      5635.0   \n",
       "mean       ...       1.418564e-18    1.525744e-16    1.008756e-17         0.0   \n",
       "std        ...       1.270903e-01    1.000000e+00    1.000000e+00         0.0   \n",
       "min        ...      -6.347297e-01   -1.114391e+00   -9.871867e-01         0.0   \n",
       "25%        ...       0.000000e+00   -5.875971e-01   -9.871867e-01         0.0   \n",
       "50%        ...       0.000000e+00   -1.925014e-01   -1.585799e-01         0.0   \n",
       "75%        ...       0.000000e+00    2.849059e-01    3.938247e-01         0.0   \n",
       "max        ...       4.498908e+00    8.104509e+00    6.194072e+00         0.0   \n",
       "\n",
       "        colon_count  ellipse_count  lparen_count   quote_count  \\\n",
       "count  5.635000e+03   5.635000e+03        5635.0  5.635000e+03   \n",
       "mean   1.040280e-17  -3.152364e-18           0.0  6.809105e-17   \n",
       "std    1.000000e+00   1.000000e+00           0.0  1.000000e+00   \n",
       "min   -1.429028e-01  -3.088445e-02           0.0 -2.089280e-01   \n",
       "25%   -1.429028e-01  -3.088445e-02           0.0 -2.089280e-01   \n",
       "50%   -1.429028e-01  -3.088445e-02           0.0 -2.089280e-01   \n",
       "75%   -1.429028e-01  -3.088445e-02           0.0 -2.089280e-01   \n",
       "max    1.887892e+01   4.693064e+01           0.0  2.887605e+01   \n",
       "\n",
       "       semicolon_count      sent_len  \n",
       "count     5.635000e+03  5.635000e+03  \n",
       "mean     -1.197898e-17 -4.287214e-17  \n",
       "std       1.000000e+00  1.000000e+00  \n",
       "min      -3.297344e-01 -1.122274e+00  \n",
       "25%      -3.297344e-01 -6.953480e-01  \n",
       "50%      -3.297344e-01 -2.498604e-01  \n",
       "75%      -3.297344e-01  3.998090e-01  \n",
       "max       2.067601e+01  1.291059e+01  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eap_gram_feats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strangely enough, grepping through the raw input indeed shows that no bang characters exist\n",
    "# the boxplots indicate that the grammatical features indeed don't seem to have much \n",
    "# predictive power, so we'll try other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection, training, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data marshalling and pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each, remove and review score. 5 runs total\n",
    "\n",
    "# accepts: dataframes\n",
    "# returns: ndarrays\n",
    "def assemble_data(pipeline):\n",
    "    def multijoin(dfs):\n",
    "        agg = dfs.pop(0)\n",
    "        for df in dfs:\n",
    "            agg = agg.join(df)\n",
    "        return agg\n",
    "    \n",
    "    X_train = multijoin(pipeline['train']).values\n",
    "    X_val = multijoin(pipeline['val']).values\n",
    "    X_test = multijoin(pipeline['test']).values\n",
    "    \n",
    "#     X_train.to_pickle('data/train_all.pickle')\n",
    "#     X_val.to_pickle('data/val_all.pickle')\n",
    "#     X_test.to_pickle('data/test_all.pickle')\n",
    "\n",
    "    assert(X_train.shape[1] == X_val.shape[1] == X_test.shape[1])\n",
    "    return {'train': X_train, 'val': X_val, 'test': X_test}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_run(X_train, Y_train, X_val, Y_val, X_test, Y_test):\n",
    "    lin_clf = LinearSVC()\n",
    "    lin_clf.fit(X_train, Y_train) \n",
    "    \n",
    "    preds = lin_clf.predict(X_val)\n",
    "    accuracy = Eval.get_accuracy(preds, Y_val)\n",
    "    print(\"Val Accuracy: \", accuracy)\n",
    "    \n",
    "    preds = lin_clf.predict(X_test)\n",
    "    accuracy = Eval.get_accuracy(preds, Y_test)\n",
    "    print(\"Test Accuracy: \", accuracy)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_n_run(X_train, Y_train, X_val, Y_val, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing for NN\n",
    "encoder = sklearn.preprocessing.LabelEncoder()\n",
    "encoder.fit(traindata.author)\n",
    "\n",
    "Y_train_nn = tf.keras.utils.to_categorical(encoder.transform(traindata.author))\n",
    "Y_val_nn   = tf.keras.utils.to_categorical(encoder.transform(valdata.author))\n",
    "Y_test_nn  = tf.keras.utils.to_categorical(encoder.transform(testdata.author))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_mdl_simple():\n",
    "    # inspired by keras docs example: https://www.tensorflow.org/guide/keras#input_numpy_data\n",
    "\n",
    "    # simple NN - but from a BOW perspective.\n",
    "    # we hypothesize that signal is to be recovered from the sequence of features,\n",
    "    # so we'll try RNNs next\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "    # Adds a densely-connected layer with 64 units to the model:\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    # Add another:\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    # Add a softmax layer with 10 output units:\n",
    "    tf.keras.layers.Dense(3, activation='softmax')])\n",
    "\n",
    "    # Configure a model for categorical classification.\n",
    "    model.compile(optimizer=tf.train.RMSPropOptimizer(0.01),\n",
    "                  loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  metrics=[tf.keras.metrics.categorical_accuracy]\n",
    "                  )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15664/15664 [==============================] - 5s 338us/step - loss: 0.8322 - categorical_accuracy: 0.6146\n",
      "Epoch 2/20\n",
      "15664/15664 [==============================] - 5s 296us/step - loss: 0.5775 - categorical_accuracy: 0.7659\n",
      "Epoch 3/20\n",
      "15664/15664 [==============================] - 5s 292us/step - loss: 0.5107 - categorical_accuracy: 0.7970\n",
      "Epoch 4/20\n",
      "15664/15664 [==============================] - 5s 297us/step - loss: 0.4642 - categorical_accuracy: 0.8204\n",
      "Epoch 5/20\n",
      "15664/15664 [==============================] - 5s 295us/step - loss: 0.4251 - categorical_accuracy: 0.8395\n",
      "Epoch 6/20\n",
      "15664/15664 [==============================] - 5s 301us/step - loss: 0.3837 - categorical_accuracy: 0.8551\n",
      "Epoch 7/20\n",
      "15664/15664 [==============================] - 6s 403us/step - loss: 0.3499 - categorical_accuracy: 0.8680\n",
      "Epoch 8/20\n",
      "15664/15664 [==============================] - 5s 329us/step - loss: 0.3273 - categorical_accuracy: 0.8866\n",
      "Epoch 9/20\n",
      "15664/15664 [==============================] - 5s 308us/step - loss: 0.2863 - categorical_accuracy: 0.8984\n",
      "Epoch 10/20\n",
      "15664/15664 [==============================] - 5s 304us/step - loss: 0.2682 - categorical_accuracy: 0.9103\n",
      "Epoch 11/20\n",
      "15664/15664 [==============================] - 5s 309us/step - loss: 0.2454 - categorical_accuracy: 0.9203\n",
      "Epoch 12/20\n",
      "15664/15664 [==============================] - 5s 320us/step - loss: 0.2157 - categorical_accuracy: 0.9314\n",
      "Epoch 13/20\n",
      "15664/15664 [==============================] - 5s 326us/step - loss: 0.2083 - categorical_accuracy: 0.9379\n",
      "Epoch 14/20\n",
      "15664/15664 [==============================] - 5s 312us/step - loss: 0.1847 - categorical_accuracy: 0.9435\n",
      "Epoch 15/20\n",
      "15664/15664 [==============================] - 5s 295us/step - loss: 0.1639 - categorical_accuracy: 0.9505\n",
      "Epoch 16/20\n",
      "15664/15664 [==============================] - 5s 293us/step - loss: 0.1727 - categorical_accuracy: 0.9538\n",
      "Epoch 17/20\n",
      "15664/15664 [==============================] - 5s 289us/step - loss: 0.1625 - categorical_accuracy: 0.9558\n",
      "Epoch 18/20\n",
      "15664/15664 [==============================] - 5s 297us/step - loss: 0.1291 - categorical_accuracy: 0.9634\n",
      "Epoch 19/20\n",
      "15664/15664 [==============================] - 5s 296us/step - loss: 0.1605 - categorical_accuracy: 0.9642\n",
      "Epoch 20/20\n",
      "15664/15664 [==============================] - 5s 293us/step - loss: 0.1388 - categorical_accuracy: 0.9690\n",
      "EAP EAP\n",
      "{'correct': 1384, 'incorrect': 574}\n",
      "NN 1Accuracy:  0.7068437180796732\n",
      "Epoch 1/40\n",
      "15664/15664 [==============================] - 5s 334us/step - loss: 0.8239 - categorical_accuracy: 0.6221\n",
      "Epoch 2/40\n",
      "15664/15664 [==============================] - 5s 289us/step - loss: 0.5751 - categorical_accuracy: 0.7696\n",
      "Epoch 3/40\n",
      "15664/15664 [==============================] - 5s 300us/step - loss: 0.5129 - categorical_accuracy: 0.7981\n",
      "Epoch 4/40\n",
      "15664/15664 [==============================] - 5s 289us/step - loss: 0.4636 - categorical_accuracy: 0.8216\n",
      "Epoch 5/40\n",
      "15664/15664 [==============================] - 5s 294us/step - loss: 0.4203 - categorical_accuracy: 0.8405\n",
      "Epoch 6/40\n",
      "15664/15664 [==============================] - 5s 288us/step - loss: 0.3817 - categorical_accuracy: 0.8569\n",
      "Epoch 7/40\n",
      "15664/15664 [==============================] - 5s 294us/step - loss: 0.3487 - categorical_accuracy: 0.8717\n",
      "Epoch 8/40\n",
      "15664/15664 [==============================] - 5s 296us/step - loss: 0.3171 - categorical_accuracy: 0.8843\n",
      "Epoch 9/40\n",
      "15664/15664 [==============================] - 5s 296us/step - loss: 0.2919 - categorical_accuracy: 0.8940\n",
      "Epoch 10/40\n",
      "15664/15664 [==============================] - 5s 293us/step - loss: 0.2715 - categorical_accuracy: 0.9079\n",
      "Epoch 11/40\n",
      "15664/15664 [==============================] - 5s 290us/step - loss: 0.2496 - categorical_accuracy: 0.9158\n",
      "Epoch 12/40\n",
      "15664/15664 [==============================] - 5s 322us/step - loss: 0.2202 - categorical_accuracy: 0.9277\n",
      "Epoch 13/40\n",
      "15664/15664 [==============================] - 5s 319us/step - loss: 0.2314 - categorical_accuracy: 0.9332\n",
      "Epoch 14/40\n",
      "15664/15664 [==============================] - 5s 294us/step - loss: 0.2139 - categorical_accuracy: 0.9383\n",
      "Epoch 15/40\n",
      "15664/15664 [==============================] - 5s 336us/step - loss: 0.1905 - categorical_accuracy: 0.9469\n",
      "Epoch 16/40\n",
      "15664/15664 [==============================] - 5s 304us/step - loss: 0.2067 - categorical_accuracy: 0.9473\n",
      "Epoch 17/40\n",
      "15664/15664 [==============================] - 5s 304us/step - loss: 0.1606 - categorical_accuracy: 0.9573\n",
      "Epoch 18/40\n",
      "15664/15664 [==============================] - 5s 327us/step - loss: 0.1605 - categorical_accuracy: 0.9604\n",
      "Epoch 19/40\n",
      "15664/15664 [==============================] - 5s 319us/step - loss: 0.1595 - categorical_accuracy: 0.9609\n",
      "Epoch 20/40\n",
      "15664/15664 [==============================] - 5s 305us/step - loss: 0.1704 - categorical_accuracy: 0.9637\n",
      "Epoch 21/40\n",
      "15664/15664 [==============================] - 4s 287us/step - loss: 0.1565 - categorical_accuracy: 0.9676\n",
      "Epoch 22/40\n",
      "15664/15664 [==============================] - 5s 299us/step - loss: 0.1539 - categorical_accuracy: 0.9699\n",
      "Epoch 23/40\n",
      "15664/15664 [==============================] - 5s 294us/step - loss: 0.1439 - categorical_accuracy: 0.9714\n",
      "Epoch 24/40\n",
      "15664/15664 [==============================] - 5s 291us/step - loss: 0.1621 - categorical_accuracy: 0.9729\n",
      "Epoch 25/40\n",
      "15664/15664 [==============================] - 5s 296us/step - loss: 0.1218 - categorical_accuracy: 0.9752\n",
      "Epoch 26/40\n",
      "15664/15664 [==============================] - 5s 288us/step - loss: 0.1560 - categorical_accuracy: 0.9737\n",
      "Epoch 27/40\n",
      "15664/15664 [==============================] - 5s 292us/step - loss: 0.1259 - categorical_accuracy: 0.9768\n",
      "Epoch 28/40\n",
      "15664/15664 [==============================] - 4s 286us/step - loss: 0.1201 - categorical_accuracy: 0.9790\n",
      "Epoch 29/40\n",
      "15664/15664 [==============================] - 5s 295us/step - loss: 0.1366 - categorical_accuracy: 0.9791\n",
      "Epoch 30/40\n",
      "15664/15664 [==============================] - 5s 301us/step - loss: 0.1311 - categorical_accuracy: 0.9790\n",
      "Epoch 31/40\n",
      "15664/15664 [==============================] - 5s 315us/step - loss: 0.1372 - categorical_accuracy: 0.9798\n",
      "Epoch 32/40\n",
      "15664/15664 [==============================] - 5s 299us/step - loss: 0.1069 - categorical_accuracy: 0.9833\n",
      "Epoch 33/40\n",
      "15664/15664 [==============================] - 5s 297us/step - loss: 0.1212 - categorical_accuracy: 0.9818\n",
      "Epoch 34/40\n",
      "15664/15664 [==============================] - 5s 296us/step - loss: 0.1246 - categorical_accuracy: 0.9824\n",
      "Epoch 35/40\n",
      "15664/15664 [==============================] - 5s 292us/step - loss: 0.1349 - categorical_accuracy: 0.9808\n",
      "Epoch 36/40\n",
      "15664/15664 [==============================] - 5s 291us/step - loss: 0.1327 - categorical_accuracy: 0.9805\n",
      "Epoch 37/40\n",
      "15664/15664 [==============================] - 5s 301us/step - loss: 0.1106 - categorical_accuracy: 0.9834\n",
      "Epoch 38/40\n",
      "15664/15664 [==============================] - 5s 292us/step - loss: 0.0987 - categorical_accuracy: 0.9852\n",
      "Epoch 39/40\n",
      "15664/15664 [==============================] - 5s 292us/step - loss: 0.1115 - categorical_accuracy: 0.9838\n",
      "Epoch 40/40\n",
      "15664/15664 [==============================] - 5s 303us/step - loss: 0.0857 - categorical_accuracy: 0.9865\n",
      "EAP EAP\n",
      "{'correct': 1414, 'incorrect': 544}\n",
      "NN 1Accuracy:  0.7221654749744637\n",
      "Epoch 1/80\n",
      "15664/15664 [==============================] - 5s 324us/step - loss: 0.8302 - categorical_accuracy: 0.6200\n",
      "Epoch 2/80\n",
      "15664/15664 [==============================] - 4s 287us/step - loss: 0.5828 - categorical_accuracy: 0.7671\n",
      "Epoch 3/80\n",
      "15664/15664 [==============================] - 5s 288us/step - loss: 0.5066 - categorical_accuracy: 0.8013\n",
      "Epoch 4/80\n",
      "15664/15664 [==============================] - 4s 285us/step - loss: 0.4606 - categorical_accuracy: 0.8214\n",
      "Epoch 5/80\n",
      "12544/15664 [=======================>......] - ETA: 0s - loss: 0.4188 - categorical_accuracy: 0.8405"
     ]
    }
   ],
   "source": [
    "#all\n",
    "# pipeline = { \\\n",
    "#     'train': [train_gram_seq_df, train_text_feats_df, train_gensim_df], \n",
    "#     'val': [val_gram_seq_df, val_text_feats_df, val_gensim_df], \n",
    "#     'test': [test_gram_seq_df, test_text_feats_df, test_gensim_df]}            \n",
    "\n",
    "# X = {}\n",
    "# X['train'] = pd.read_pickle('data/train_all.pickle').values\n",
    "# X['val'] = pd.read_pickle('data/val_all.pickle').values\n",
    "# X['test'] = pd.read_pickle('data/test_all.pickle').values\n",
    "#X = assemble_data(pipeline)\n",
    "\n",
    "#no seq\n",
    "pipeline2 = { \\\n",
    "    'train': [train_gram_df, train_text_feats_df, train_gensim_df], \n",
    "    'val': [val_gram_df, val_text_feats_df, val_gensim_df], \n",
    "    'test': [test_gram_df, test_text_feats_df, test_gensim_df]}  \n",
    "\n",
    "# no text\n",
    "pipeline3 = { \\\n",
    "    'train': [train_gram_df, train_gensim_df], \n",
    "    'val': [val_gram_df, val_gensim_df], \n",
    "    'test': [test_gram_df, test_gensim_df]}  \n",
    "\n",
    "# no gensim\n",
    "pipeline4 = { \\\n",
    "    'train': [train_gram_df, train_text_feats_df], \n",
    "    'val': [val_gram_df, val_text_feats_df], \n",
    "    'test': [test_gram_df, test_text_feats_df]}  \n",
    "\n",
    "\n",
    "\n",
    "for pipeline in [pipeline2]:\n",
    "    X = assemble_data(pipeline)\n",
    "    \n",
    "    model = nn_mdl_simple()\n",
    "    model.fit(X['train'], Y_train_nn, epochs=20, batch_size=32)\n",
    "    numerical_preds = model.predict(X['val'], verbose=2 )\n",
    "    Eval.nn_accuracy(encoder.classes_, numerical_preds, Y_val)    \n",
    "    \n",
    "    model = nn_mdl_simple()\n",
    "    model.fit(X['train'], Y_train_nn, epochs=40, batch_size=32)\n",
    "    numerical_preds = model.predict(X['val'], verbose=2 )\n",
    "    Eval.nn_accuracy(encoder.classes_, numerical_preds, Y_val)  \n",
    "\n",
    "    model = nn_mdl_simple()\n",
    "    model.fit(X['train'], Y_train_nn, epochs=80, batch_size=32)\n",
    "    numerical_preds = model.predict(X['val'], verbose=2 )\n",
    "    Eval.nn_accuracy(encoder.classes_, numerical_preds, Y_val) \n",
    "    \n",
    "    model = nn_mdl_simple()\n",
    "    model.fit(X['train'], Y_train_nn, epochs=160, batch_size=32)\n",
    "    numerical_preds = model.predict(X['val'], verbose=2 )\n",
    "    Eval.nn_accuracy(encoder.classes_, numerical_preds, Y_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2 # super great! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_nn_preds(classes, preds):\n",
    "    label_lookup = {}\n",
    "    for idx, label in enumerate(classes):\n",
    "        label_lookup[idx] = label\n",
    "    label_lookup\n",
    "\n",
    "    def best_label(row, lookup):\n",
    "        mx = max(row)\n",
    "        return lookup[list(row).index(mx)]\n",
    "\n",
    "    preds = [best_label(x, label_lookup) for x in preds]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# error analysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "preds = format_nn_preds(encoder.classes_, numerical_preds)\n",
    "conf_mat = confusion_matrix(Y_test, preds)\n",
    "\n",
    "\n",
    "# NOTE: this function taken from: \n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = 500\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plot_confusion_matrix(conf_mat, classes=encoder.classes_,\n",
    "                      title=\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS: NN confusse MWS with EAP more than other combos\n",
    "\n",
    "# [good place to insert val vs. test metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logtime()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
