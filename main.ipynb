{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/peterkong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# local code\n",
    "sys.path.insert(1, \"./code/\")\n",
    "from Utils import Utils # student's library\n",
    "from Eval import Eval # student's library\n",
    "from Extract import Extract # student's library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "#train_df.id.nunique()\n",
    "# 19579\n",
    "\n",
    "#train_df.author.unique()\n",
    "# array(['EAP', 'HPL', 'MWS'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 0, 'id': 0, 'text': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Utils.check_for_nulls(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data\n",
    "\n",
    "# regular data\n",
    "#     train: 19580 * .9 rows\n",
    "#     test:  8393 rows\n",
    "#     val:   19580 * .1 rows\n",
    "\n",
    "\n",
    "if os.path.isfile('data/traindata.pickle'):\n",
    "    traindata = pd.read_pickle('data/traindata.pickle')\n",
    "    valdata   = pd.read_pickle('data/valdata.pickle')\n",
    "    testdata  = pd.read_pickle('data/testdata.pickle')\n",
    "else: \n",
    "    VAL_IDX  = math.ceil(len(train_df) * .8)\n",
    "    TEST_IDX = math.ceil(len(train_df) * .9)\n",
    "\n",
    "    traindata = train_df[:VAL_IDX]\n",
    "    valdata   = train_df[VAL_IDX:TEST_IDX]\n",
    "    testdata  = train_df[TEST_IDX:]\n",
    "\n",
    "    print(VAL_IDX, TEST_IDX)\n",
    "\n",
    "    traindata.to_pickle('data/traindata.pickle')\n",
    "    valdata.to_pickle('data/valdata.pickle')\n",
    "    testdata.to_pickle('data/testdata.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindata: 15664, valdata: 1958, testdata: 1957\n"
     ]
    }
   ],
   "source": [
    "print(\"traindata: {}, valdata: {}, testdata: {}\".format(len(traindata), len(valdata), len(testdata)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at:  Sun Dec  9 17:10:51 2018\n"
     ]
    }
   ],
   "source": [
    "def logtime():\n",
    "    timenow = time.asctime( time.localtime(time.time()) )\n",
    "    print(\"Finished at: \", timenow)\n",
    "logtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "Y_train = list(traindata.author )\n",
    "Y_val = list(valdata.author)\n",
    "Y_test = list(testdata.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading gram feats from pickle\n",
      "Finished at:  Sun Dec  9 17:10:52 2018\n"
     ]
    }
   ],
   "source": [
    "# grammatical feature engineering \n",
    "# we want to include stopwords here\n",
    "\n",
    "def gen_gram_feats(name, seq_no, train, val, test):\n",
    "    train_df = Extract.gram_feats(train.text, None, seq_no)\n",
    "\n",
    "    # need to remember so that val/test process\n",
    "    # does not add additional columns\n",
    "    gram_feat_list = list(train_df.columns)\n",
    "\n",
    "    val_df = Extract.gram_feats(val.text, gram_feat_list, seq_no)\n",
    "    test_df = Extract.gram_feats(test.text, gram_feat_list, seq_no)\n",
    "\n",
    "    # removes a singleton feature\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        if 'SYM_count' in list(df.columns):\n",
    "            df.drop('SYM_count', axis=1, inplace=True)\n",
    "        \n",
    "    train_df.to_pickle('data/train_' + name + '_df.pickle')\n",
    "    val_df.to_pickle('data/val_' + name + '_df.pickle')\n",
    "    test_df.to_pickle('data/test_' + name + '_df.pickle')       \n",
    "\n",
    "\n",
    "if os.path.isfile('data/train_gram_df.pickle'):\n",
    "    print(\"reading gram feats from pickle\")\n",
    "    # pass\n",
    "else:\n",
    "    print(\"writing gram feats pickles\")\n",
    "    gen_gram_feats('gram', None, traindata, valdata, testdata)\n",
    "    gen_gram_feats('gram_seq', 7, traindata, valdata, testdata)\n",
    "    \n",
    "train_gram_df = pd.read_pickle('data/train_gram_df.pickle')\n",
    "val_gram_df   = pd.read_pickle('data/val_gram_df.pickle')\n",
    "test_gram_df  = pd.read_pickle('data/test_gram_df.pickle')\n",
    "\n",
    "train_gram_seq_df = pd.read_pickle('data/train_gram_seq_df.pickle')\n",
    "val_gram_seq_df   = pd.read_pickle('data/val_gram_seq_df.pickle')\n",
    "test_gram_seq_df  = pd.read_pickle('data/test_gram_seq_df.pickle')\n",
    "    \n",
    "logtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15664, 23)\n",
      "(1958, 23)\n",
      "(1957, 23)\n",
      "(15664, 1622)\n",
      "(1958, 1622)\n",
      "(1957, 1622)\n"
     ]
    }
   ],
   "source": [
    "print(train_gram_df.shape)\n",
    "print(val_gram_df.shape)\n",
    "print(test_gram_df.shape)\n",
    "\n",
    "print(train_gram_seq_df.shape)\n",
    "print(val_gram_seq_df.shape)\n",
    "print(test_gram_seq_df.shape)\n",
    "\n",
    "assert(train_gram_df.shape == (15664, 23))\n",
    "assert(train_gram_seq_df.shape == (15664, 1622))\n",
    "#set(GRAM_FEAT_LIST) - set(list(val_gram_feats_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ADJ_count</th>\n",
       "      <th>ADP_count</th>\n",
       "      <th>ADV_count</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>CCONJ_CCONJ_CCONJ_VERB_NOUN</th>\n",
       "      <th>CCONJ_CCONJ_NOUN</th>\n",
       "      <th>CCONJ_CCONJ_NOUN_CCONJ_NOUN_CCONJ_NOUN</th>\n",
       "      <th>CCONJ_CCONJ_NOUN_CCONJ_NOUN_NOUN_VERB</th>\n",
       "      <th>CCONJ_CCONJ_NOUN_CCONJ_NOUN_VERB_VERB</th>\n",
       "      <th>...</th>\n",
       "      <th>X_count</th>\n",
       "      <th>adj_noun_ratio</th>\n",
       "      <th>adv_verb_ratio</th>\n",
       "      <th>bang_count</th>\n",
       "      <th>colon_count</th>\n",
       "      <th>ellipse_count</th>\n",
       "      <th>lparen_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>semicolon_count</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235080</td>\n",
       "      <td>0.513819</td>\n",
       "      <td>0.627603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363031</td>\n",
       "      <td>0.277382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1611</td>\n",
       "      <td>-0.018801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.237475</td>\n",
       "      <td>2.918125</td>\n",
       "      <td>0.762854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.737871</td>\n",
       "      <td>-0.660999</td>\n",
       "      <td>0.084256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.191755</td>\n",
       "      <td>-0.288286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1611</td>\n",
       "      <td>-0.018801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.237475</td>\n",
       "      <td>-0.440646</td>\n",
       "      <td>-0.726940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.208031</td>\n",
       "      <td>0.513819</td>\n",
       "      <td>-0.459090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.199733</td>\n",
       "      <td>0.923860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1611</td>\n",
       "      <td>-0.018801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.237475</td>\n",
       "      <td>-0.440646</td>\n",
       "      <td>0.474207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.883714</td>\n",
       "      <td>0.513819</td>\n",
       "      <td>0.084256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.493349</td>\n",
       "      <td>0.439002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1611</td>\n",
       "      <td>-0.018801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.237475</td>\n",
       "      <td>-0.440646</td>\n",
       "      <td>0.530074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235080</td>\n",
       "      <td>-0.367294</td>\n",
       "      <td>1.170950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.150797</td>\n",
       "      <td>-0.409501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1611</td>\n",
       "      <td>-0.018801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.237475</td>\n",
       "      <td>1.238739</td>\n",
       "      <td>0.232115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1622 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ADJ_count  ADP_count  ADV_count  CCONJ  CCONJ_CCONJ_CCONJ_VERB_NOUN  \\\n",
       "0  0.0   0.235080   0.513819   0.627603    0.0                          0.0   \n",
       "1  0.0  -0.737871  -0.660999   0.084256    0.0                          0.0   \n",
       "2  0.0   1.208031   0.513819  -0.459090    0.0                          0.0   \n",
       "3  0.0   0.883714   0.513819   0.084256    0.0                          0.0   \n",
       "4  0.0   0.235080  -0.367294   1.170950    0.0                          0.0   \n",
       "\n",
       "   CCONJ_CCONJ_NOUN  CCONJ_CCONJ_NOUN_CCONJ_NOUN_CCONJ_NOUN  \\\n",
       "0               0.0                                     0.0   \n",
       "1               0.0                                     0.0   \n",
       "2               0.0                                     0.0   \n",
       "3               0.0                                     0.0   \n",
       "4               0.0                                     0.0   \n",
       "\n",
       "   CCONJ_CCONJ_NOUN_CCONJ_NOUN_NOUN_VERB  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   CCONJ_CCONJ_NOUN_CCONJ_NOUN_VERB_VERB    ...     X_count  adj_noun_ratio  \\\n",
       "0                                    0.0    ...         0.0        0.363031   \n",
       "1                                    0.0    ...         0.0        0.191755   \n",
       "2                                    0.0    ...         0.0       -0.199733   \n",
       "3                                    0.0    ...         0.0       -0.493349   \n",
       "4                                    0.0    ...         0.0       -0.150797   \n",
       "\n",
       "   adv_verb_ratio  bang_count  colon_count  ellipse_count  lparen_count  \\\n",
       "0        0.277382         0.0      -0.1611      -0.018801           0.0   \n",
       "1       -0.288286         0.0      -0.1611      -0.018801           0.0   \n",
       "2        0.923860         0.0      -0.1611      -0.018801           0.0   \n",
       "3        0.439002         0.0      -0.1611      -0.018801           0.0   \n",
       "4       -0.409501         0.0      -0.1611      -0.018801           0.0   \n",
       "\n",
       "   quote_count  semicolon_count  sent_len  \n",
       "0    -0.237475         2.918125  0.762854  \n",
       "1    -0.237475        -0.440646 -0.726940  \n",
       "2    -0.237475        -0.440646  0.474207  \n",
       "3    -0.237475        -0.440646  0.530074  \n",
       "4    -0.237475         1.238739  0.232115  \n",
       "\n",
       "[5 rows x 1622 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gram_seq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15664, 1500)\n",
      "(1958, 1500)\n",
      "(1957, 1500)\n"
     ]
    }
   ],
   "source": [
    "# textual feature engineering\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=STOPWORDS, max_features=1500)\n",
    "\n",
    "train_text_feats = vectorizer.fit_transform(traindata.text)\n",
    "val_text_feats = vectorizer.transform(valdata.text) \n",
    "test_text_feats = vectorizer.transform(testdata.text) \n",
    "\n",
    "cols = [\"text_\" + str(x) for x in range(train_text_feats.shape[1])]\n",
    "\n",
    "train_text_feats_df = pd.DataFrame(train_text_feats.todense(), index=None, columns=cols)\n",
    "val_text_feats_df = pd.DataFrame(val_text_feats.todense(), index=None, columns=cols)\n",
    "test_text_feats_df = pd.DataFrame(test_text_feats.todense(), index=None, columns=cols)\n",
    "\n",
    "print(train_text_feats_df.shape)\n",
    "print(val_text_feats_df.shape)\n",
    "print(test_text_feats_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist to disk\n",
    "# if not os.path.isfile('data/train_text_feats_df.pickle'):\n",
    "#     train_text_feats_df.to_pickle('data/train_text_feats_df.pickle')\n",
    "#     val_text_feats_df.to_pickle('data/val_text_feats_df.pickle')\n",
    "#     test_text_feats_df.to_pickle('data/test_text_feats_df.pickle')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_0</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>text_3</th>\n",
       "      <th>text_4</th>\n",
       "      <th>text_5</th>\n",
       "      <th>text_6</th>\n",
       "      <th>text_7</th>\n",
       "      <th>text_8</th>\n",
       "      <th>text_9</th>\n",
       "      <th>...</th>\n",
       "      <th>text_1490</th>\n",
       "      <th>text_1491</th>\n",
       "      <th>text_1492</th>\n",
       "      <th>text_1493</th>\n",
       "      <th>text_1494</th>\n",
       "      <th>text_1495</th>\n",
       "      <th>text_1496</th>\n",
       "      <th>text_1497</th>\n",
       "      <th>text_1498</th>\n",
       "      <th>text_1499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_0  text_1  text_2  text_3  text_4  text_5  text_6  text_7  text_8  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   text_9    ...      text_1490  text_1491  text_1492  text_1493  text_1494  \\\n",
       "0     0.0    ...            0.0        0.0        0.0        0.0   0.000000   \n",
       "1     0.0    ...            0.0        0.0        0.0        0.0   0.000000   \n",
       "2     0.0    ...            0.0        0.0        0.0        0.0   0.000000   \n",
       "3     0.0    ...            0.0        0.0        0.0        0.0   0.225799   \n",
       "4     0.0    ...            0.0        0.0        0.0        0.0   0.000000   \n",
       "\n",
       "   text_1495  text_1496  text_1497  text_1498  text_1499  \n",
       "0        0.0        0.0        0.0        0.0        0.0  \n",
       "1        0.0        0.0        0.0        0.0        0.0  \n",
       "2        0.0        0.0        0.0        0.0        0.0  \n",
       "3        0.0        0.0        0.0        0.0        0.0  \n",
       "4        0.0        0.0        0.0        0.0        0.0  \n",
       "\n",
       "[5 rows x 1500 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_feats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at:  Sun Dec  9 17:10:53 2018\n",
      "Finished at:  Sun Dec  9 17:11:13 2018\n"
     ]
    }
   ],
   "source": [
    "# gensim feature engineering\n",
    "import numpy as np\n",
    "import gensim\n",
    "GENSIM = True\n",
    "\n",
    "if GENSIM:\n",
    "    logtime()\n",
    "    #https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "    from gensim.test.utils import common_texts\n",
    "    from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "    #TaggedDocument does not filter or stem\n",
    "\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(list(traindata.text))]\n",
    "    model = Doc2Vec(documents, vector_size=1500, window=2, min_count=1, workers=4)\n",
    "\n",
    "    train_gensim = np.array([model.infer_vector(x) for x in list(traindata.text)])\n",
    "    val_gensim = np.array([model.infer_vector(x) for x in list(valdata.text)])\n",
    "    test_gensim = np.array([model.infer_vector(x) for x in list(testdata.text)])\n",
    "\n",
    "    # numpy to pandas\n",
    "    cols = [\"gensim_\" + str(x) for x in range(len(train_gensim[0]))]\n",
    "\n",
    "    train_gensim_df = pd.DataFrame(train_gensim, index=None, columns=cols)\n",
    "    val_gensim_df = pd.DataFrame(val_gensim, index=None, columns=cols)\n",
    "    test_gensim_df = pd.DataFrame(test_gensim, index=None, columns=cols)\n",
    "    logtime()\n",
    "    \n",
    "# gensim didn't help. so we're settling on tfidf extual features for now, and will explore neural models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lex = Utils.build_lexicon(traindata.text, STOPWORDS)\n",
    "# len(lex)\n",
    "\n",
    "# 22847 different tokens in full lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJ_count</th>\n",
       "      <th>ADP_count</th>\n",
       "      <th>ADV_count</th>\n",
       "      <th>CCONJ_count</th>\n",
       "      <th>DET_count</th>\n",
       "      <th>INTJ_count</th>\n",
       "      <th>NOUN_count</th>\n",
       "      <th>NUM_count</th>\n",
       "      <th>PART_count</th>\n",
       "      <th>PRON_count</th>\n",
       "      <th>...</th>\n",
       "      <th>X_count</th>\n",
       "      <th>adj_noun_ratio</th>\n",
       "      <th>adv_verb_ratio</th>\n",
       "      <th>bang_count</th>\n",
       "      <th>colon_count</th>\n",
       "      <th>ellipse_count</th>\n",
       "      <th>lparen_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>semicolon_count</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.043782e-18</td>\n",
       "      <td>6.777582e-17</td>\n",
       "      <td>2.395796e-17</td>\n",
       "      <td>-7.187389e-17</td>\n",
       "      <td>9.457091e-18</td>\n",
       "      <td>2.521891e-18</td>\n",
       "      <td>4.980734e-17</td>\n",
       "      <td>1.670753e-17</td>\n",
       "      <td>1.891418e-18</td>\n",
       "      <td>4.224167e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>7.880909e-20</td>\n",
       "      <td>1.387040e-17</td>\n",
       "      <td>4.539403e-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.120840e-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.639229e-17</td>\n",
       "      <td>-7.565672e-18</td>\n",
       "      <td>-3.152364e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.609892e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.428109e-01</td>\n",
       "      <td>9.289644e-01</td>\n",
       "      <td>1.797328e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.101639e-01</td>\n",
       "      <td>6.326520e-01</td>\n",
       "      <td>9.163641e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>5.807221e-02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-9.669821e-01</td>\n",
       "      <td>-8.362820e-01</td>\n",
       "      <td>-8.746468e-01</td>\n",
       "      <td>-6.749403e-01</td>\n",
       "      <td>-7.230532e-01</td>\n",
       "      <td>-2.697985e-01</td>\n",
       "      <td>-1.032234e+00</td>\n",
       "      <td>-3.935250e-01</td>\n",
       "      <td>-5.018364e-01</td>\n",
       "      <td>-6.978646e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.874173e-01</td>\n",
       "      <td>-1.238327e+00</td>\n",
       "      <td>-9.793490e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.231806e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.097765e-01</td>\n",
       "      <td>-5.894256e-01</td>\n",
       "      <td>-1.017688e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.949294e-01</td>\n",
       "      <td>-5.730821e-01</td>\n",
       "      <td>-8.746468e-01</td>\n",
       "      <td>-6.749403e-01</td>\n",
       "      <td>-3.758420e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-4.800391e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-5.018364e-01</td>\n",
       "      <td>-6.978646e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-4.909876e-01</td>\n",
       "      <td>-9.793490e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.231806e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.097765e-01</td>\n",
       "      <td>-5.894256e-01</td>\n",
       "      <td>-5.281714e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.508239e-01</td>\n",
       "      <td>-4.668209e-02</td>\n",
       "      <td>-3.236535e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.863082e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.119090e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.920516e-01</td>\n",
       "      <td>-1.519656e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.231806e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.097765e-01</td>\n",
       "      <td>-5.894256e-01</td>\n",
       "      <td>-1.785166e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.932816e-01</td>\n",
       "      <td>2.165179e-01</td>\n",
       "      <td>2.273397e-01</td>\n",
       "      <td>9.130644e-02</td>\n",
       "      <td>3.185804e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.562212e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000804e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.563523e-01</td>\n",
       "      <td>4.685719e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.231806e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.097765e-01</td>\n",
       "      <td>7.428701e-01</td>\n",
       "      <td>3.110000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.059114e+01</td>\n",
       "      <td>3.311652e+01</td>\n",
       "      <td>2.171608e+01</td>\n",
       "      <td>1.924748e+01</td>\n",
       "      <td>3.226201e+01</td>\n",
       "      <td>7.693608e+00</td>\n",
       "      <td>3.449232e+01</td>\n",
       "      <td>7.238050e+00</td>\n",
       "      <td>1.794448e+01</td>\n",
       "      <td>2.443740e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.023842e+00</td>\n",
       "      <td>9.224431e+00</td>\n",
       "      <td>6.467101e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.138564e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.594267e+01</td>\n",
       "      <td>7.404349e+00</td>\n",
       "      <td>3.505114e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ADJ_count     ADP_count     ADV_count   CCONJ_count     DET_count  \\\n",
       "count  5.635000e+03  5.635000e+03  5.635000e+03  5.635000e+03  5.635000e+03   \n",
       "mean   5.043782e-18  6.777582e-17  2.395796e-17 -7.187389e-17  9.457091e-18   \n",
       "std    1.000000e+00  9.609892e-01  1.000000e+00  8.428109e-01  9.289644e-01   \n",
       "min   -9.669821e-01 -8.362820e-01 -8.746468e-01 -6.749403e-01 -7.230532e-01   \n",
       "25%   -6.949294e-01 -5.730821e-01 -8.746468e-01 -6.749403e-01 -3.758420e-01   \n",
       "50%   -1.508239e-01 -4.668209e-02 -3.236535e-01  0.000000e+00 -2.863082e-02   \n",
       "75%    3.932816e-01  2.165179e-01  2.273397e-01  9.130644e-02  3.185804e-01   \n",
       "max    3.059114e+01  3.311652e+01  2.171608e+01  1.924748e+01  3.226201e+01   \n",
       "\n",
       "         INTJ_count    NOUN_count     NUM_count    PART_count    PRON_count  \\\n",
       "count  5.635000e+03  5.635000e+03  5.635000e+03  5.635000e+03  5.635000e+03   \n",
       "mean   2.521891e-18  4.980734e-17  1.670753e-17  1.891418e-18  4.224167e-17   \n",
       "std    1.797328e-01  1.000000e+00  3.101639e-01  6.326520e-01  9.163641e-01   \n",
       "min   -2.697985e-01 -1.032234e+00 -3.935250e-01 -5.018364e-01 -6.978646e-01   \n",
       "25%    0.000000e+00 -4.800391e-01  0.000000e+00 -5.018364e-01 -6.978646e-01   \n",
       "50%    0.000000e+00 -1.119090e-01  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  2.562212e-01  0.000000e+00  0.000000e+00  1.000804e-01   \n",
       "max    7.693608e+00  3.449232e+01  7.238050e+00  1.794448e+01  2.443740e+01   \n",
       "\n",
       "           ...            X_count  adj_noun_ratio  adv_verb_ratio  bang_count  \\\n",
       "count      ...       5.635000e+03    5.635000e+03    5.635000e+03      5635.0   \n",
       "mean       ...       7.880909e-20    1.387040e-17    4.539403e-17         0.0   \n",
       "std        ...       5.807221e-02    1.000000e+00    1.000000e+00         0.0   \n",
       "min        ...      -2.874173e-01   -1.238327e+00   -9.793490e-01         0.0   \n",
       "25%        ...       0.000000e+00   -4.909876e-01   -9.793490e-01         0.0   \n",
       "50%        ...       0.000000e+00   -1.920516e-01   -1.519656e-01         0.0   \n",
       "75%        ...       0.000000e+00    2.563523e-01    4.685719e-01         0.0   \n",
       "max        ...       4.023842e+00    9.224431e+00    6.467101e+00         0.0   \n",
       "\n",
       "        colon_count  ellipse_count  lparen_count   quote_count  \\\n",
       "count  5.635000e+03         5635.0        5635.0  5.635000e+03   \n",
       "mean  -3.120840e-17            0.0           0.0  1.639229e-17   \n",
       "std    1.000000e+00            0.0           0.0  1.000000e+00   \n",
       "min   -2.231806e-01            0.0           0.0 -2.097765e-01   \n",
       "25%   -2.231806e-01            0.0           0.0 -2.097765e-01   \n",
       "50%   -2.231806e-01            0.0           0.0 -2.097765e-01   \n",
       "75%   -2.231806e-01            0.0           0.0 -2.097765e-01   \n",
       "max    1.138564e+01            0.0           0.0  2.594267e+01   \n",
       "\n",
       "       semicolon_count      sent_len  \n",
       "count     5.635000e+03  5.635000e+03  \n",
       "mean     -7.565672e-18 -3.152364e-17  \n",
       "std       1.000000e+00  1.000000e+00  \n",
       "min      -5.894256e-01 -1.017688e+00  \n",
       "25%      -5.894256e-01 -5.281714e-01  \n",
       "50%      -5.894256e-01 -1.785166e-01  \n",
       "75%       7.428701e-01  3.110000e-01  \n",
       "max       7.404349e+00  3.505114e+01  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mws_df = train_df[train_df.author == 'MWS']\n",
    "hpl_df = train_df[train_df.author == 'HPL']\n",
    "eap_df = train_df[train_df.author == 'EAP']\n",
    "\n",
    "cutoff = min([mws_df.shape[0], hpl_df.shape[0], eap_df.shape[0]])\n",
    "\n",
    "# equalize corpus sizes to avoid bias during exploration\n",
    "mws_df = mws_df[:cutoff]\n",
    "hpl_df = hpl_df[:cutoff]\n",
    "eap_df = eap_df[:cutoff]\n",
    "\n",
    "mws_lexicon = Utils.build_lexicon(mws_df.text, STOPWORDS)\n",
    "hpl_lexicon = Utils.build_lexicon(hpl_df.text, STOPWORDS)\n",
    "eap_lexicon = Utils.build_lexicon(eap_df.text, STOPWORDS)\n",
    "\n",
    "# sanity check\n",
    "assert(cutoff * 3 == len(mws_df) + len(hpl_df) + len(eap_df))\n",
    "\n",
    "# add grammatical features (for exploration this time, not training)\n",
    "mws_gram_feats_df = Extract.gram_feats(mws_df.text, None, None)\n",
    "hpl_gram_feats_df = Extract.gram_feats(hpl_df.text, None, None)\n",
    "eap_gram_feats_df = Extract.gram_feats(eap_df.text, None, None)\n",
    "\n",
    "mws_gram_feats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like sentence length values are consistently higher by at least a degree of magnitude\n",
    "# so we'll take the log\n",
    "# This is not done via the standardize() method\n",
    "\n",
    "# for df in [mws_gram_feats_df, hpl_gram_feats_df, eap_gram_feats_df]:\n",
    "#     df['sent_len'] = df['sent_len'].apply(lambda x: math.log(x))\n",
    "#     df.rename(inplace=True, columns={'sent_len': 'log_sent_len'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER exploration\n",
    "\n",
    "# avoiding NER for three reasons:\n",
    "# features very sparse\n",
    "# features seem content-specific, so may contribute to misprediction\n",
    "# should we add data from the same authors about other topics\n",
    "\n",
    "NER = False\n",
    "if NER:\n",
    "    import spacy\n",
    "    spacy_mdl = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def sent_to_ents(sent, spacy):\n",
    "        sent = spacy(sent)\n",
    "        ents = []\n",
    "        for ent in sent.ents:\n",
    "            ents.append(ent.text + ':' + ent.label_)\n",
    "        return ents\n",
    "\n",
    "    entities = []\n",
    "    for i in range(1500):\n",
    "        sent = valdata.iloc[i].text\n",
    "        ents = sent_to_ents(sent, spacy_mdl)\n",
    "        entities.append(ents)\n",
    " \n",
    "\n",
    "# example entities list HERE\n",
    "# many sentences don't have any entities, like below\n",
    "#'In whatever way the shifting is managed, it is of course concealed at every step from observation.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NER:\n",
    "    import statistics as stat\n",
    "    entity_freq = [len(x) for x in entities]\n",
    "\n",
    "\n",
    "    print(\"stats for NER within a sample group: \\n\")\n",
    "    print(\"min: {} \\nmax: {} \\nmean: {} \\nstdev: {}\" \\\n",
    "          .format(min(entity_freq), max(entity_freq), stat.mean(entity_freq), stat.stdev(entity_freq)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration - visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example MWS sentence: \n",
      "3    How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.\n",
      "\n",
      "Example HPL sentence: \n",
      "1    It never once occurred to me that the fumbling might be a mere mistake.\n",
      "\n",
      "Example EAP sentence: \n",
      "0    This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data viz\n",
    "\n",
    "def plot_word_freq(lexicon, name, quantity=20):\n",
    "    plt.rcdefaults()\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    elems = [x[0] for x in lexicon[:quantity]]\n",
    "    y_pos = np.arange(quantity)\n",
    "    vals = [x[1] for x in lexicon[:quantity]]\n",
    "\n",
    "    ax.barh(y_pos, vals, align='center',\n",
    "            color='green', ecolor='black')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(elems)\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_xlabel('Corpus-wide frequency')\n",
    "    ax.set_title(name + ' - Word Frequencies')\n",
    "\n",
    "    plt.show()\n",
    " \n",
    "authors = {'MWS': mws_lexicon, 'HPL': hpl_lexicon, 'EAP': eap_lexicon}\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(\"Example MWS sentence: \\n{}\\n\".format(mws_df.text[:1].to_string()))\n",
    "print(\"Example HPL sentence: \\n{}\\n\".format(hpl_df.text[:1].to_string()))\n",
    "print(\"Example EAP sentence: \\n{}\\n\".format(eap_df.text[:1].to_string()))\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "\n",
    "# for key in authors: \n",
    "#     plot_word_freq(authors[key], key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(df, subset):\n",
    "    features = {\n",
    "        'tag_features': [\n",
    "             'ADJ_count',\n",
    "             'ADP_count',\n",
    "             'ADV_count',\n",
    "             'CCONJ_count',\n",
    "             'DET_count',\n",
    "             'NOUN_count',\n",
    "             'PRON_count',\n",
    "             'VERB_count'],\n",
    "        'punc_features': [\n",
    "            'bang_count',\n",
    "            'colon_count',\n",
    "            'ellipse_count',\n",
    "            'lparen_count',\n",
    "            'quote_count',\n",
    "            'semicolon_count'],\n",
    "        'ratio_features': [\n",
    "             'adj_noun_ratio',\n",
    "             'adv_verb_ratio',\n",
    "             'log_sent_len']\n",
    "    }\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    boxplot = df.boxplot(column=features[subset], \\\n",
    "        showfliers=False, fontsize=6, figsize=None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(mws_gram_feats_df, 'ratio_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(hpl_gram_feats_df, 'ratio_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(eap_gram_feats_df, 'ratio_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(mws_gram_feats_df, 'tag_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(hpl_gram_feats_df, 'tag_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(eap_gram_feats_df, 'tag_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(mws_gram_feats_df, 'punc_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(hpl_gram_feats_df, 'punc_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(eap_gram_feats_df, 'punc_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJ_count</th>\n",
       "      <th>ADP_count</th>\n",
       "      <th>ADV_count</th>\n",
       "      <th>CCONJ_count</th>\n",
       "      <th>DET_count</th>\n",
       "      <th>INTJ_count</th>\n",
       "      <th>NOUN_count</th>\n",
       "      <th>NUM_count</th>\n",
       "      <th>PART_count</th>\n",
       "      <th>PRON_count</th>\n",
       "      <th>...</th>\n",
       "      <th>X_count</th>\n",
       "      <th>adj_noun_ratio</th>\n",
       "      <th>adv_verb_ratio</th>\n",
       "      <th>bang_count</th>\n",
       "      <th>colon_count</th>\n",
       "      <th>ellipse_count</th>\n",
       "      <th>lparen_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>semicolon_count</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "      <td>5.635000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.278458e-17</td>\n",
       "      <td>3.782836e-18</td>\n",
       "      <td>-3.908931e-17</td>\n",
       "      <td>4.413309e-17</td>\n",
       "      <td>9.330996e-17</td>\n",
       "      <td>-7.880909e-19</td>\n",
       "      <td>9.330996e-17</td>\n",
       "      <td>-1.008756e-17</td>\n",
       "      <td>-3.026269e-17</td>\n",
       "      <td>-1.052889e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.418564e-18</td>\n",
       "      <td>1.525744e-16</td>\n",
       "      <td>1.008756e-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.040280e-17</td>\n",
       "      <td>-3.152364e-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.809105e-17</td>\n",
       "      <td>-1.197898e-17</td>\n",
       "      <td>-4.287214e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.436558e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>7.660251e-01</td>\n",
       "      <td>9.423383e-01</td>\n",
       "      <td>1.747253e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.663131e-01</td>\n",
       "      <td>5.880118e-01</td>\n",
       "      <td>8.508802e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.270903e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.023842e+00</td>\n",
       "      <td>-9.239556e-01</td>\n",
       "      <td>-9.682718e-01</td>\n",
       "      <td>-5.978215e-01</td>\n",
       "      <td>-9.095897e-01</td>\n",
       "      <td>-4.410888e-01</td>\n",
       "      <td>-1.177239e+00</td>\n",
       "      <td>-3.619786e-01</td>\n",
       "      <td>-5.497347e-01</td>\n",
       "      <td>-7.627328e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.347297e-01</td>\n",
       "      <td>-1.114391e+00</td>\n",
       "      <td>-9.871867e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.429028e-01</td>\n",
       "      <td>-3.088445e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.089280e-01</td>\n",
       "      <td>-3.297344e-01</td>\n",
       "      <td>-1.122274e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.729724e-01</td>\n",
       "      <td>-6.505018e-01</td>\n",
       "      <td>-4.682982e-01</td>\n",
       "      <td>-5.978215e-01</td>\n",
       "      <td>-5.604881e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-7.168663e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-7.627328e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-5.875971e-01</td>\n",
       "      <td>-9.871867e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.429028e-01</td>\n",
       "      <td>-3.088445e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.089280e-01</td>\n",
       "      <td>-3.297344e-01</td>\n",
       "      <td>-6.953480e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-3.221027e-01</td>\n",
       "      <td>-1.035942e-01</td>\n",
       "      <td>-4.682982e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.113865e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.564934e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-8.636901e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.925014e-01</td>\n",
       "      <td>-1.585799e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.429028e-01</td>\n",
       "      <td>-3.088445e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.089280e-01</td>\n",
       "      <td>-3.297344e-01</td>\n",
       "      <td>-2.498604e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.796366e-01</td>\n",
       "      <td>1.698596e-01</td>\n",
       "      <td>5.316489e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.377151e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.340658e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.849059e-01</td>\n",
       "      <td>3.938247e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.429028e-01</td>\n",
       "      <td>-3.088445e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.089280e-01</td>\n",
       "      <td>-3.297344e-01</td>\n",
       "      <td>3.998090e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.266008e+01</td>\n",
       "      <td>1.630363e+01</td>\n",
       "      <td>8.531226e+00</td>\n",
       "      <td>9.254316e+00</td>\n",
       "      <td>1.445088e+01</td>\n",
       "      <td>6.587313e+00</td>\n",
       "      <td>1.424525e+01</td>\n",
       "      <td>1.828845e+01</td>\n",
       "      <td>8.596645e+00</td>\n",
       "      <td>8.706361e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.498908e+00</td>\n",
       "      <td>8.104509e+00</td>\n",
       "      <td>6.194072e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.887892e+01</td>\n",
       "      <td>4.693064e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.887605e+01</td>\n",
       "      <td>2.067601e+01</td>\n",
       "      <td>1.291059e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ADJ_count     ADP_count     ADV_count   CCONJ_count     DET_count  \\\n",
       "count  5.635000e+03  5.635000e+03  5.635000e+03  5.635000e+03  5.635000e+03   \n",
       "mean  -3.278458e-17  3.782836e-18 -3.908931e-17  4.413309e-17  9.330996e-17   \n",
       "std    1.000000e+00  9.436558e-01  1.000000e+00  7.660251e-01  9.423383e-01   \n",
       "min   -1.023842e+00 -9.239556e-01 -9.682718e-01 -5.978215e-01 -9.095897e-01   \n",
       "25%   -6.729724e-01 -6.505018e-01 -4.682982e-01 -5.978215e-01 -5.604881e-01   \n",
       "50%   -3.221027e-01 -1.035942e-01 -4.682982e-01  0.000000e+00 -2.113865e-01   \n",
       "75%    3.796366e-01  1.698596e-01  5.316489e-01  0.000000e+00  1.377151e-01   \n",
       "max    1.266008e+01  1.630363e+01  8.531226e+00  9.254316e+00  1.445088e+01   \n",
       "\n",
       "         INTJ_count    NOUN_count     NUM_count    PART_count    PRON_count  \\\n",
       "count  5.635000e+03  5.635000e+03  5.635000e+03  5.635000e+03  5.635000e+03   \n",
       "mean  -7.880909e-19  9.330996e-17 -1.008756e-17 -3.026269e-17 -1.052889e-16   \n",
       "std    1.747253e-01  1.000000e+00  3.663131e-01  5.880118e-01  8.508802e-01   \n",
       "min   -4.410888e-01 -1.177239e+00 -3.619786e-01 -5.497347e-01 -7.627328e-01   \n",
       "25%    0.000000e+00 -7.168663e-01  0.000000e+00  0.000000e+00 -7.627328e-01   \n",
       "50%    0.000000e+00 -2.564934e-01  0.000000e+00  0.000000e+00 -8.636901e-02   \n",
       "75%    0.000000e+00  4.340658e-01  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    6.587313e+00  1.424525e+01  1.828845e+01  8.596645e+00  8.706361e+00   \n",
       "\n",
       "           ...            X_count  adj_noun_ratio  adv_verb_ratio  bang_count  \\\n",
       "count      ...       5.635000e+03    5.635000e+03    5.635000e+03      5635.0   \n",
       "mean       ...       1.418564e-18    1.525744e-16    1.008756e-17         0.0   \n",
       "std        ...       1.270903e-01    1.000000e+00    1.000000e+00         0.0   \n",
       "min        ...      -6.347297e-01   -1.114391e+00   -9.871867e-01         0.0   \n",
       "25%        ...       0.000000e+00   -5.875971e-01   -9.871867e-01         0.0   \n",
       "50%        ...       0.000000e+00   -1.925014e-01   -1.585799e-01         0.0   \n",
       "75%        ...       0.000000e+00    2.849059e-01    3.938247e-01         0.0   \n",
       "max        ...       4.498908e+00    8.104509e+00    6.194072e+00         0.0   \n",
       "\n",
       "        colon_count  ellipse_count  lparen_count   quote_count  \\\n",
       "count  5.635000e+03   5.635000e+03        5635.0  5.635000e+03   \n",
       "mean   1.040280e-17  -3.152364e-18           0.0  6.809105e-17   \n",
       "std    1.000000e+00   1.000000e+00           0.0  1.000000e+00   \n",
       "min   -1.429028e-01  -3.088445e-02           0.0 -2.089280e-01   \n",
       "25%   -1.429028e-01  -3.088445e-02           0.0 -2.089280e-01   \n",
       "50%   -1.429028e-01  -3.088445e-02           0.0 -2.089280e-01   \n",
       "75%   -1.429028e-01  -3.088445e-02           0.0 -2.089280e-01   \n",
       "max    1.887892e+01   4.693064e+01           0.0  2.887605e+01   \n",
       "\n",
       "       semicolon_count      sent_len  \n",
       "count     5.635000e+03  5.635000e+03  \n",
       "mean     -1.197898e-17 -4.287214e-17  \n",
       "std       1.000000e+00  1.000000e+00  \n",
       "min      -3.297344e-01 -1.122274e+00  \n",
       "25%      -3.297344e-01 -6.953480e-01  \n",
       "50%      -3.297344e-01 -2.498604e-01  \n",
       "75%      -3.297344e-01  3.998090e-01  \n",
       "max       2.067601e+01  1.291059e+01  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eap_gram_feats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strangely enough, grepping through the raw input indeed shows that no bang characters exist\n",
    "# the boxplots indicate that the grammatical features indeed don't seem to have much \n",
    "# predictive power, so we'll try other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection, training, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data marshalling and pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each, remove and review score. 5 runs total\n",
    "\n",
    "# accepts: dataframes\n",
    "# returns: ndarrays\n",
    "def assemble_data(pipeline):\n",
    "    def multijoin(dfs):\n",
    "        agg = dfs.pop(0)\n",
    "        for df in dfs:\n",
    "            agg = agg.join(df)\n",
    "        return agg\n",
    "    \n",
    "    X_train = multijoin(pipeline['train']).values\n",
    "    X_val = multijoin(pipeline['val']).values\n",
    "    X_test = multijoin(pipeline['test']).values\n",
    "\n",
    "    assert(X_train.shape[1] == X_val.shape[1] == X_test.shape[1])\n",
    "    return {'train': X_train, 'val': X_val, 'test': X_test}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_gensim_df.shape\n",
    "#1622, 1500, 1500\n",
    "#type(pipelines['all']['test'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_run(X_train, Y_train, X_val, Y_val, X_test, Y_test):\n",
    "    lin_clf = LinearSVC()\n",
    "    lin_clf.fit(X_train, Y_train) \n",
    "    \n",
    "    preds = lin_clf.predict(X_val)\n",
    "    accuracy = Eval.get_accuracy(preds, Y_val)\n",
    "    print(\"Val Accuracy: \", accuracy)\n",
    "    \n",
    "    preds = lin_clf.predict(X_test)\n",
    "    accuracy = Eval.get_accuracy(preds, Y_test)\n",
    "    print(\"Test Accuracy: \", accuracy)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_n_run(X_train, Y_train, X_val, Y_val, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing for NN\n",
    "encoder = sklearn.preprocessing.LabelEncoder()\n",
    "encoder.fit(traindata.author)\n",
    "\n",
    "Y_train_nn = tf.keras.utils.to_categorical(encoder.transform(traindata.author))\n",
    "Y_val_nn   = tf.keras.utils.to_categorical(encoder.transform(valdata.author))\n",
    "Y_test_nn  = tf.keras.utils.to_categorical(encoder.transform(testdata.author))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_mdl_simple():\n",
    "    # inspired by keras docs example: https://www.tensorflow.org/guide/keras#input_numpy_data\n",
    "\n",
    "    # simple NN - but from a BOW perspective.\n",
    "    # we hypothesize that signal is to be recovered from the sequence of features,\n",
    "    # so we'll try RNNs next\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "    # Adds a densely-connected layer with 64 units to the model:\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    # Add another:\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    # Add a softmax layer with 10 output units:\n",
    "    tf.keras.layers.Dense(3, activation='softmax')])\n",
    "\n",
    "    # Configure a model for categorical classification.\n",
    "    model.compile(optimizer=tf.train.RMSPropOptimizer(0.01),\n",
    "                  loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  metrics=[tf.keras.metrics.categorical_accuracy]\n",
    "                  )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = { \\\n",
    "    'train': [train_gram_seq_df, train_text_feats_df, train_gensim_df], \n",
    "    'val': [val_gram_seq_df, val_text_feats_df, val_gensim_df], \n",
    "    'test': [val_gram_seq_df, val_text_feats_df, val_gensim_df]}            \n",
    "\n",
    "\n",
    "X = assemble_data(pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_text_feats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15664 samples, validate on 1958 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model = nn_mdl_simple()\n",
    "model.fit(X['train'], Y_train_nn, epochs=10, batch_size=32, validation_data=(X['val'], Y_val_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns loss, accuracy\n",
    "numerical_preds = model.predict(X['test'], verbose=2 )\n",
    "Eval.nn_accuracy(encoder.classes_, numerical_preds, Y_test_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error analysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "conf_mat = confusion_matrix(Y_val, preds)\n",
    "\n",
    "\n",
    "# NOTE: this function taken from: \n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = 500\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "print(conf_mat)\n",
    "plot_confusion_matrix(conf_mat, classes=lin_clf.classes_,\n",
    "                      title=\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [good place to insert val vs. test metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logtime()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
