{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/peterkong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# local code\n",
    "sys.path.insert(1, \"./code/\")\n",
    "from Utils import Utils # student's library\n",
    "from Eval import Eval # student's library\n",
    "from Extract import Extract # student's library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "#train_df.id.nunique()\n",
    "# 19579\n",
    "\n",
    "#train_df.author.unique()\n",
    "# array(['EAP', 'HPL', 'MWS'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 0, 'id': 0, 'text': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Utils.check_for_nulls(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_df = train_df[:20]\n",
    "short_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data\n",
    "\n",
    "# regular data\n",
    "#     train: 19580 * .9 rows\n",
    "#     test:  8393 rows\n",
    "#     val:   19580 * .1 rows\n",
    "\n",
    "\n",
    "if os.path.isfile('data/traindata.pickle'):\n",
    "    traindata = pd.read_pickle('data/traindata.pickle')\n",
    "    valdata   = pd.read_pickle('data/valdata.pickle')\n",
    "    testdata  = pd.read_pickle('data/testdata.pickle')\n",
    "else: \n",
    "    VAL_IDX  = math.ceil(len(train_df) * .8)\n",
    "    TEST_IDX = math.ceil(len(train_df) * .9)\n",
    "\n",
    "    traindata = train_df[:VAL_IDX]\n",
    "    valdata   = train_df[VAL_IDX:TEST_IDX]\n",
    "    testdata  = train_df[TEST_IDX:]\n",
    "\n",
    "    print(VAL_IDX, TEST_IDX)\n",
    "\n",
    "    traindata.to_pickle('data/traindata.pickle')\n",
    "    valdata.to_pickle('data/valdata.pickle')\n",
    "    testdata.to_pickle('data/testdata.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindata: 15664, valdata: 1958, testdata: 1957\n"
     ]
    }
   ],
   "source": [
    "print(\"traindata: {}, valdata: {}, testdata: {}\".format(len(traindata), len(valdata), len(testdata)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammatical feature engineering \n",
    "# we want to include stopwords here\n",
    "\n",
    "if os.path.isfile('data/train_gram_feats.pickle'):\n",
    "    print(\"reading gram feats from pickle\")\n",
    "    train_gram_feats_df = pd.read_pickle('data/train_gram_feats_df.pickle')\n",
    "    val_gram_feats_df   = pd.read_pickle('data/val_gram_feats_df.pickle')\n",
    "    test_gram_feats_df  = pd.read_pickle('data/test_gram_feats_df.pickle')\n",
    "else:\n",
    "    seq_no = None\n",
    "    train_gram_feats_df = Extract.gram_feats(traindata.text, None, seq_no)\n",
    "\n",
    "    # need to remember so that val/test process\n",
    "    # does not add additional columns\n",
    "    GRAM_FEAT_LIST = list(train_gram_feats_df.columns)\n",
    "\n",
    "    val_gram_feats_df = Extract.gram_feats(valdata.text, GRAM_FEAT_LIST, seq_no)\n",
    "    test_gram_feats_df = Extract.gram_feats(testdata.text, GRAM_FEAT_LIST, seq_no)\n",
    "\n",
    "    # there are 21 columns excluding sequence columns \n",
    "    # 7ary sequence columns can generate up to 2187\n",
    "    \n",
    "    train_gram_feats_df.to_pickle('data/train_gram_feats_df.pickle')\n",
    "    val_gram_feats_df.to_pickle('data/val_gram_feats_df.pickle')\n",
    "    test_gram_feats_df.to_pickle('data/test_gram_feats_df.pickle')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15664, 23)\n",
      "(1958, 23)\n",
      "(1957, 23)\n"
     ]
    }
   ],
   "source": [
    "# removes a singleton feature\n",
    "for df in train_gram_feats_df, val_gram_feats_df, test_gram_feats_df:\n",
    "    if 'SYM_count' in list(df.columns):\n",
    "        df.drop('SYM_count', axis=1, inplace=True)\n",
    "        \n",
    "print(train_gram_feats_df.shape)\n",
    "print(val_gram_feats_df.shape)\n",
    "print(test_gram_feats_df.shape)\n",
    "\n",
    "#set(GRAM_FEAT_LIST) - set(list(val_gram_feats_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15664, 23)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gram_feats_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textual feature engineering\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=STOPWORDS, max_features=100)\n",
    "\n",
    "train_text_feats = vectorizer.fit_transform(traindata.text)\n",
    "Y_train = traindata.author \n",
    "\n",
    "val_text_feats = vectorizer.transform(valdata.text) \n",
    "Y_val = list(valdata.author)\n",
    "\n",
    "test_text_feats = vectorizer.transform(testdata.text) \n",
    "Y_test = list(testdata.author)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.matrixlib.defmatrix.matrix"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " type(test_text_feats.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15664, 100)\n",
      "(1958, 100)\n",
      "(1957, 100)\n"
     ]
    }
   ],
   "source": [
    "#convert text feats to pandas\n",
    "\n",
    "cols = [\"text_\" + str(x) for x in range(train_text_feats.shape[1])]\n",
    "\n",
    "train_text_feats_df = pd.DataFrame(train_text_feats.todense(), index=None, columns=cols)\n",
    "val_text_feats_df = pd.DataFrame(val_text_feats.todense(), index=None, columns=cols)\n",
    "test_text_feats_df = pd.DataFrame(test_text_feats.todense(), index=None, columns=cols)\n",
    "\n",
    "print(train_text_feats_df.shape)\n",
    "print(val_text_feats_df.shape)\n",
    "print(test_text_feats_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist to disk\n",
    "if not os.path.isfile('data/train_text_feats_df.pickle'):\n",
    "    train_text_feats_df.to_pickle('data/train_text_feats_df.pickle')\n",
    "    val_text_feats_df.to_pickle('data/val_text_feats_df.pickle')\n",
    "    test_text_feats_df.to_pickle('data/test_text_feats_df.pickle')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15664, 100)\n",
      "(1958, 100)\n",
      "(1957, 100)\n"
     ]
    }
   ],
   "source": [
    "# concatenating features\n",
    "\n",
    "    #traindata.text.to_frame() for gensim\n",
    "    #val_gram_feats_df.fillna(0)\n",
    "X_train = train_text_feats_df\n",
    "X_val = val_text_feats_df.fillna(0)\n",
    "X_test = test_text_feats_df.fillna(0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame({'foo':[1,3,99], 'bar':[2,4,7]})\n",
    "\n",
    "def transform(df):\n",
    "    def xform(x, **kwargs):\n",
    "        return (x - avg) / stdv\n",
    "\n",
    "    for col in df.columns:\n",
    "        stdv = df[col].std()\n",
    "        avg = df[col].mean()\n",
    "        df[col] = df[col].apply(xform, avg=avg, stdv=stdv)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell throws on non numerical columns\n",
    "\n",
    "# X_train = transform(X_train).fillna(0)\n",
    "# X_val = transform(X_val).fillna(0)\n",
    "# X_test = transform(X_test).fillna(0)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_val.shape)\n",
    "# print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity checking\n",
    "assert(list(X_train.columns) == list(X_val.columns))\n",
    "assert(list(X_train.columns) == list(X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(w2v.keys())[9990:9999]\n",
    "# [b'iconic',\n",
    "#  b'erp',\n",
    "#  b'crest',\n",
    "#  b'radius',\n",
    "#  b'spiral',\n",
    "#  b'nyse',\n",
    "#  b'lotion',\n",
    "#  b'oriental',\n",
    "#  b'admire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_key = list(w2v.keys())[9999]\n",
    "# a_key.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(w2v[str.encode('owl')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import Extract\n",
    "# import Utils\n",
    "# importlib.reload(Extract)\n",
    "# importlib.reload(Utils)\n",
    "# from Utils import Utils\n",
    "#from Extract import Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJ_count</th>\n",
       "      <th>ADP_count</th>\n",
       "      <th>ADV_count</th>\n",
       "      <th>CCONJ_count</th>\n",
       "      <th>DET_count</th>\n",
       "      <th>INTJ_count</th>\n",
       "      <th>NOUN_count</th>\n",
       "      <th>NUM_count</th>\n",
       "      <th>PART_count</th>\n",
       "      <th>PRON_count</th>\n",
       "      <th>...</th>\n",
       "      <th>X_count</th>\n",
       "      <th>adj_noun_ratio</th>\n",
       "      <th>adv_verb_ratio</th>\n",
       "      <th>bang_count</th>\n",
       "      <th>colon_count</th>\n",
       "      <th>ellipse_count</th>\n",
       "      <th>lparen_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>semicolon_count</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5204.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>4003.000000</td>\n",
       "      <td>4863.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>543.000000</td>\n",
       "      <td>2256.000000</td>\n",
       "      <td>4732.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.554392</td>\n",
       "      <td>4.177364</td>\n",
       "      <td>1.587400</td>\n",
       "      <td>1.880839</td>\n",
       "      <td>3.082459</td>\n",
       "      <td>1.169399</td>\n",
       "      <td>5.607986</td>\n",
       "      <td>1.206262</td>\n",
       "      <td>1.435284</td>\n",
       "      <td>2.749155</td>\n",
       "      <td>...</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.656980</td>\n",
       "      <td>2.367340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080213</td>\n",
       "      <td>0.442413</td>\n",
       "      <td>151.974800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.675758</td>\n",
       "      <td>3.799392</td>\n",
       "      <td>1.814904</td>\n",
       "      <td>1.305063</td>\n",
       "      <td>2.880091</td>\n",
       "      <td>0.627872</td>\n",
       "      <td>5.432861</td>\n",
       "      <td>0.524138</td>\n",
       "      <td>0.867382</td>\n",
       "      <td>2.506438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.695852</td>\n",
       "      <td>1.338079</td>\n",
       "      <td>2.417259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.258424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.382373</td>\n",
       "      <td>0.750584</td>\n",
       "      <td>128.698383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>129.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>192.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>116.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4663.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ADJ_count    ADP_count    ADV_count  CCONJ_count    DET_count  \\\n",
       "count  5635.000000  5204.000000  5635.000000  4003.000000  4863.000000   \n",
       "mean      3.554392     4.177364     1.587400     1.880839     3.082459   \n",
       "std       3.675758     3.799392     1.814904     1.305063     2.880091   \n",
       "min       0.000000     1.000000     0.000000     1.000000     1.000000   \n",
       "25%       1.000000     2.000000     0.000000     1.000000     1.000000   \n",
       "50%       3.000000     3.000000     1.000000     2.000000     2.000000   \n",
       "75%       5.000000     5.000000     2.000000     2.000000     4.000000   \n",
       "max     116.000000   130.000000    41.000000    27.000000    96.000000   \n",
       "\n",
       "       INTJ_count   NOUN_count   NUM_count   PART_count   PRON_count  \\\n",
       "count  183.000000  5635.000000  543.000000  2256.000000  4732.000000   \n",
       "mean     1.169399     5.607986    1.206262     1.435284     2.749155   \n",
       "std      0.627872     5.432861    0.524138     0.867382     2.506438   \n",
       "min      1.000000     0.000000    1.000000     1.000000     1.000000   \n",
       "25%      1.000000     3.000000    1.000000     1.000000     1.000000   \n",
       "50%      1.000000     5.000000    1.000000     1.000000     2.000000   \n",
       "75%      1.000000     7.000000    1.000000     2.000000     4.000000   \n",
       "max      6.000000   193.000000    5.000000    17.000000    64.000000   \n",
       "\n",
       "          ...         X_count  adj_noun_ratio  adv_verb_ratio  bang_count  \\\n",
       "count     ...       20.000000     5635.000000     5635.000000      5635.0   \n",
       "mean      ...        1.200000        1.656980        2.367340         0.0   \n",
       "std       ...        0.695852        1.338079        2.417259         0.0   \n",
       "min       ...        1.000000        0.000000        0.000000         0.0   \n",
       "25%       ...        1.000000        1.000000        0.000000         0.0   \n",
       "50%       ...        1.000000        1.400000        2.000000         0.0   \n",
       "75%       ...        1.000000        2.000000        3.500000         0.0   \n",
       "max       ...        4.000000       14.000000       18.000000         0.0   \n",
       "\n",
       "       colon_count  ellipse_count  lparen_count  quote_count  semicolon_count  \\\n",
       "count  5635.000000         5635.0        5635.0  5635.000000      5635.000000   \n",
       "mean      0.057675            0.0           0.0     0.080213         0.442413   \n",
       "std       0.258424            0.0           0.0     0.382373         0.750584   \n",
       "min       0.000000            0.0           0.0     0.000000         0.000000   \n",
       "25%       0.000000            0.0           0.0     0.000000         0.000000   \n",
       "50%       0.000000            0.0           0.0     0.000000         0.000000   \n",
       "75%       0.000000            0.0           0.0     0.000000         1.000000   \n",
       "max       3.000000            0.0           0.0    10.000000         6.000000   \n",
       "\n",
       "          sent_len  \n",
       "count  5635.000000  \n",
       "mean    151.974800  \n",
       "std     128.698383  \n",
       "min      21.000000  \n",
       "25%      84.000000  \n",
       "50%     129.000000  \n",
       "75%     192.000000  \n",
       "max    4663.000000  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exploration\n",
    "\n",
    "\n",
    "mws_df = train_df[train_df.author == 'MWS']\n",
    "hpl_df = train_df[train_df.author == 'HPL']\n",
    "eap_df = train_df[train_df.author == 'EAP']\n",
    "\n",
    "cutoff = min([mws_df.shape[0], hpl_df.shape[0], eap_df.shape[0]])\n",
    "\n",
    "# equalize corpus sizes to avoid bias during exploration\n",
    "mws_df = mws_df[:cutoff]\n",
    "hpl_df = hpl_df[:cutoff]\n",
    "eap_df = eap_df[:cutoff]\n",
    "\n",
    "mws_lexicon = Utils.build_lexicon(mws_df.text, STOPWORDS)\n",
    "hpl_lexicon = Utils.build_lexicon(hpl_df.text, STOPWORDS)\n",
    "eap_lexicon = Utils.build_lexicon(eap_df.text, STOPWORDS)\n",
    "\n",
    "# sanity check\n",
    "assert(cutoff * 3 == len(mws_df) + len(hpl_df) + len(eap_df))\n",
    "\n",
    "# add grammatical features (for exploration this time, not training)\n",
    "mws_gram_feats_df = Extract.gram_feats(mws_df.text, None, None)\n",
    "hpl_gram_feats_df = Extract.gram_feats(hpl_df.text, None, None)\n",
    "eap_gram_feats_df = Extract.gram_feats(eap_df.text, None, None)\n",
    "\n",
    "mws_gram_feats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like sentence length values are consistently higher by at least a degree of magnitude\n",
    "# so we'll take the log\n",
    "for df in [mws_gram_feats_df, hpl_gram_feats_df, eap_gram_feats_df]:\n",
    "    df['sent_len'] = df['sent_len'].apply(lambda x: math.log(x))\n",
    "    df.rename(inplace=True, columns={'sent_len': 'log_sent_len'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER exploration\n",
    "\n",
    "import spacy\n",
    "spacy_mdl = spacy.load('en_core_web_sm')\n",
    "\n",
    "def sent_to_ents(sent, spacy):\n",
    "    sent = spacy(sent)\n",
    "    ents = []\n",
    "    for ent in sent.ents:\n",
    "        ents.append(ent.text + ':' + ent.label_)\n",
    "    return ents\n",
    "        \n",
    "entities = []\n",
    "for i in range(1500):\n",
    "    sent = valdata.iloc[i].text\n",
    "    ents = sent_to_ents(sent, spacy_mdl)\n",
    "    entities.append(ents)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoiding NER for three reasons:\n",
    "# features very sparse\n",
    "# features seem content-specific, so may contribute to misprediction\n",
    "# should we add data from the same authors about other topics\n",
    "\n",
    "\n",
    "# example entities list HERE\n",
    "# many sentences don't have any entities, like below\n",
    "#'In whatever way the shifting is managed, it is of course concealed at every step from observation.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats for NER within a sample group: \n",
      "\n",
      "min: 0 \n",
      "max: 14 \n",
      "mean: 0.8 \n",
      "stdev: 1.2284160549271035\n"
     ]
    }
   ],
   "source": [
    "import statistics as stat\n",
    "entity_freq = [len(x) for x in entities]\n",
    "\n",
    "\n",
    "print(\"stats for NER within a sample group: \\n\")\n",
    "print(\"min: {} \\nmax: {} \\nmean: {} \\nstdev: {}\" \\\n",
    "      .format(min(entity_freq), max(entity_freq), stat.mean(entity_freq), stat.stdev(entity_freq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example MWS sentence: \n",
      "3    How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.\n",
      "\n",
      "Example HPL sentence: \n",
      "1    It never once occurred to me that the fumbling might be a mere mistake.\n",
      "\n",
      "Example EAP sentence: \n",
      "0    This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data viz\n",
    "\n",
    "def plot_word_freq(lexicon, name, quantity=20):\n",
    "    plt.rcdefaults()\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    elems = [x[0] for x in lexicon[:quantity]]\n",
    "    y_pos = np.arange(quantity)\n",
    "    vals = [x[1] for x in lexicon[:quantity]]\n",
    "\n",
    "    ax.barh(y_pos, vals, align='center',\n",
    "            color='green', ecolor='black')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(elems)\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_xlabel('Corpus-wide frequency')\n",
    "    ax.set_title(name + ' - Word Frequencies')\n",
    "\n",
    "    plt.show()\n",
    " \n",
    "authors = {'MWS': mws_lexicon, 'HPL': hpl_lexicon, 'EAP': eap_lexicon}\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(\"Example MWS sentence: \\n{}\\n\".format(mws_df.text[:1].to_string()))\n",
    "print(\"Example HPL sentence: \\n{}\\n\".format(hpl_df.text[:1].to_string()))\n",
    "print(\"Example EAP sentence: \\n{}\\n\".format(eap_df.text[:1].to_string()))\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "\n",
    "# for key in authors: \n",
    "#     plot_word_freq(authors[key], key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(df, subset):\n",
    "    features = {\n",
    "        'tag_features': [\n",
    "             'ADJ_count',\n",
    "             'ADP_count',\n",
    "             'ADV_count',\n",
    "             'CCONJ_count',\n",
    "             'DET_count',\n",
    "             'NOUN_count',\n",
    "             'PRON_count',\n",
    "             'VERB_count'],\n",
    "        'punc_features': [\n",
    "            'bang_count',\n",
    "            'colon_count',\n",
    "            'ellipse_count',\n",
    "            'lparen_count',\n",
    "            'quote_count',\n",
    "            'semicolon_count'],\n",
    "        'ratio_features': [\n",
    "             'adj_noun_ratio',\n",
    "             'adv_verb_ratio',\n",
    "             'log_sent_len']\n",
    "    }\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    boxplot = df.boxplot(column=features[subset], \\\n",
    "        showfliers=False, fontsize=6, figsize=None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(mws_gram_feats_df, 'ratio_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(hpl_gram_feats_df, 'ratio_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(eap_gram_feats_df, 'ratio_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(mws_gram_feats_df, 'tag_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(hpl_gram_feats_df, 'tag_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(eap_gram_feats_df, 'tag_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(mws_gram_feats_df, 'punc_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(hpl_gram_feats_df, 'punc_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_box(eap_gram_feats_df, 'punc_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJ_count</th>\n",
       "      <th>ADP_count</th>\n",
       "      <th>ADV_count</th>\n",
       "      <th>CCONJ_count</th>\n",
       "      <th>DET_count</th>\n",
       "      <th>INTJ_count</th>\n",
       "      <th>NOUN_count</th>\n",
       "      <th>NUM_count</th>\n",
       "      <th>PART_count</th>\n",
       "      <th>PRON_count</th>\n",
       "      <th>...</th>\n",
       "      <th>X_count</th>\n",
       "      <th>adj_noun_ratio</th>\n",
       "      <th>adv_verb_ratio</th>\n",
       "      <th>bang_count</th>\n",
       "      <th>colon_count</th>\n",
       "      <th>ellipse_count</th>\n",
       "      <th>lparen_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>semicolon_count</th>\n",
       "      <th>log_sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5018.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>3307.000000</td>\n",
       "      <td>5004.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>1949.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.0</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.918012</td>\n",
       "      <td>4.378836</td>\n",
       "      <td>1.936646</td>\n",
       "      <td>1.728152</td>\n",
       "      <td>3.605516</td>\n",
       "      <td>1.439306</td>\n",
       "      <td>5.114286</td>\n",
       "      <td>1.504624</td>\n",
       "      <td>1.420729</td>\n",
       "      <td>2.127696</td>\n",
       "      <td>...</td>\n",
       "      <td>1.989130</td>\n",
       "      <td>1.692336</td>\n",
       "      <td>1.787072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022538</td>\n",
       "      <td>0.011180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.165217</td>\n",
       "      <td>0.172671</td>\n",
       "      <td>4.710147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.850061</td>\n",
       "      <td>3.656925</td>\n",
       "      <td>2.000106</td>\n",
       "      <td>1.218010</td>\n",
       "      <td>2.864495</td>\n",
       "      <td>0.995959</td>\n",
       "      <td>4.344305</td>\n",
       "      <td>1.394070</td>\n",
       "      <td>0.765330</td>\n",
       "      <td>1.478494</td>\n",
       "      <td>...</td>\n",
       "      <td>1.558349</td>\n",
       "      <td>1.518619</td>\n",
       "      <td>1.810267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157714</td>\n",
       "      <td>0.361998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.790786</td>\n",
       "      <td>0.523666</td>\n",
       "      <td>0.710429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.044522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.204693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.744932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.220356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>7.334982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ADJ_count    ADP_count    ADV_count  CCONJ_count    DET_count  \\\n",
       "count  5635.000000  5018.000000  5635.000000  3307.000000  5004.000000   \n",
       "mean      2.918012     4.378836     1.936646     1.728152     3.605516   \n",
       "std       2.850061     3.656925     2.000106     1.218010     2.864495   \n",
       "min       0.000000     1.000000     0.000000     1.000000     1.000000   \n",
       "25%       1.000000     2.000000     1.000000     1.000000     2.000000   \n",
       "50%       2.000000     3.000000     1.000000     1.000000     3.000000   \n",
       "75%       4.000000     6.000000     3.000000     2.000000     5.000000   \n",
       "max      39.000000    64.000000    19.000000    13.000000    45.000000   \n",
       "\n",
       "       INTJ_count   NOUN_count   NUM_count   PART_count   PRON_count  \\\n",
       "count  173.000000  5635.000000  757.000000  1949.000000  4080.000000   \n",
       "mean     1.439306     5.114286    1.504624     1.420729     2.127696   \n",
       "std      0.995959     4.344305    1.394070     0.765330     1.478494   \n",
       "min      1.000000     0.000000    1.000000     1.000000     1.000000   \n",
       "25%      1.000000     2.000000    1.000000     1.000000     1.000000   \n",
       "50%      1.000000     4.000000    1.000000     1.000000     2.000000   \n",
       "75%      2.000000     7.000000    2.000000     2.000000     3.000000   \n",
       "max      8.000000    67.000000   27.000000     8.000000    15.000000   \n",
       "\n",
       "           ...         X_count  adj_noun_ratio  adv_verb_ratio  bang_count  \\\n",
       "count      ...       92.000000     5635.000000     5635.000000      5635.0   \n",
       "mean       ...        1.989130        1.692336        1.787072         0.0   \n",
       "std        ...        1.558349        1.518619        1.810267         0.0   \n",
       "min        ...        1.000000        0.000000        0.000000         0.0   \n",
       "25%        ...        1.000000        0.800000        0.000000         0.0   \n",
       "50%        ...        1.000000        1.400000        1.500000         0.0   \n",
       "75%        ...        2.000000        2.125000        2.500000         0.0   \n",
       "max        ...        9.000000       14.000000       13.000000         0.0   \n",
       "\n",
       "       colon_count  ellipse_count  lparen_count  quote_count  semicolon_count  \\\n",
       "count  5635.000000    5635.000000        5635.0  5635.000000      5635.000000   \n",
       "mean      0.022538       0.011180           0.0     0.165217         0.172671   \n",
       "std       0.157714       0.361998           0.0     0.790786         0.523666   \n",
       "min       0.000000       0.000000           0.0     0.000000         0.000000   \n",
       "25%       0.000000       0.000000           0.0     0.000000         0.000000   \n",
       "50%       0.000000       0.000000           0.0     0.000000         0.000000   \n",
       "75%       0.000000       0.000000           0.0     0.000000         0.000000   \n",
       "max       3.000000      17.000000           0.0    23.000000        11.000000   \n",
       "\n",
       "       log_sent_len  \n",
       "count   5635.000000  \n",
       "mean       4.710147  \n",
       "std        0.710429  \n",
       "min        3.044522  \n",
       "25%        4.204693  \n",
       "50%        4.744932  \n",
       "75%        5.220356  \n",
       "max        7.334982  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eap_gram_feats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strangely enough, grepping through the raw input indeed shows that no bang characters exist\n",
    "# the boxplots indicate that the grammatical features indeed don't seem to have much \n",
    "# predictive power, so we'll try other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim feature engineering\n",
    "import numpy as np\n",
    "import gensim\n",
    "GENSIM = False\n",
    "\n",
    "if GENSIM:\n",
    "\n",
    "    #https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "    from gensim.test.utils import common_texts\n",
    "    from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "    #TaggedDocument does not filter or stem\n",
    "\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(list(X_val.text))]\n",
    "    model = Doc2Vec(documents, vector_size=100, window=2, min_count=1, workers=4)\n",
    "\n",
    "    train_gensim = np.array([model.infer_vector(x) for x in list(X_train.text)])\n",
    "    val_gensim = np.array([model.infer_vector(x) for x in list(X_val.text)])\n",
    "    X_test = np.array([model.infer_vector(x) for x in list(X_test.text)])\n",
    "\n",
    "    # numpy to pandas\n",
    "    cols = [\"gensim_\" + str(x) for x in range(len(train_gensim[0]))]\n",
    "\n",
    "    X_train = pd.DataFrame(train_gensim, index=None, columns=cols)\n",
    "    X_val = pd.DataFrame(val_gensim, index=None, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EAP', 'HPL', 'HPL', ..., 'EAP', 'EAP', 'HPL'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.557711950970378\n"
     ]
    }
   ],
   "source": [
    "lin_clf = LinearSVC()\n",
    "lin_clf.fit(X_train, Y_train) \n",
    "preds = lin_clf.predict(X_val)\n",
    "preds\n",
    "\n",
    "accuracy = Eval.get_accuracy(preds, Y_val)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim didn't help. so we're settling on tfidf extual features for now, and will explore neural models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error analysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "conf_mat = confusion_matrix(Y_val, preds)\n",
    "\n",
    "\n",
    "# NOTE: this function taken from: \n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = 500\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "print(conf_mat)\n",
    "plot_confusion_matrix(conf_mat, classes=mdl.classes_,\n",
    "                      title=\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [good place to insert val vs. test metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timenow = time.asctime( time.localtime(time.time()) )\n",
    "print(\"Finished at: \", timenow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
